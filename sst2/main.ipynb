{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import json\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import remove\n",
    "from pprint import pprint\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from gensim.models import KeyedVectors\n",
    "import word2vecReader as godin_embedding\n",
    "import fasttext\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [12,10]\n",
    "from base_learners import cnn,lstm,bi_lstm,cnn_bi_lstm,cnn_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_file(filename):\n",
    "    with open(filename,'r', errors='ignore') as fin:\n",
    "        lines = fin.readlines()\n",
    "    label = [int(x.split()[0]) for x in lines]\n",
    "    sentence = [' '.join(x.split()[1:]) for x in lines]\n",
    "    return label,sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels,train_sentences = load_data_from_file('dataset/sst2/stsa.binary.train')\n",
    "dev_label,dev_sentence = load_data_from_file('dataset/sst2/stsa.binary.dev')\n",
    "test_labels,test_sentences = load_data_from_file('dataset/sst2/stsa.binary.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = train_sentences+dev_sentence\n",
    "train_labels = train_labels+dev_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7792, 7792, 1821, 1821)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels),len(train_sentences),len(test_labels),len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_classes = len(set(train_labels))\n",
    "number_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3738\n",
      "1 4054\n"
     ]
    }
   ],
   "source": [
    "for x in range(number_of_classes):\n",
    "    print(x,train_labels.count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 912\n",
      "1 909\n"
     ]
    }
   ],
   "source": [
    "for x in range(number_of_classes):\n",
    "    print(x,test_labels.count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = train_sentences[:500]\n",
    "train_labels = train_labels[:500]\n",
    "test_sentences=test_sentences[:100]\n",
    "test_labels = test_labels[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    list_punctuation = list(string.punctuation)\n",
    "    for i in list_punctuation:\n",
    "        s = s.replace(i,'')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    # should have used translate but for some reason it breaks on my server\n",
    "    tokens = [remove_punctuation(w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning data\n"
     ]
    }
   ],
   "source": [
    "print(\"cleaning data\")\n",
    "trainX = [clean_sentence(s) for s in train_sentences]\n",
    "testX = [clean_sentence(s) for s in test_sentences]\n",
    "trainY = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengths = [len(line.split()) for line in trainX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 23.,  48.,  78., 127.,  78.,  65.,  46.,  22.,   4.,   9.]),\n",
       " array([ 0. ,  2.3,  4.6,  6.9,  9.2, 11.5, 13.8, 16.1, 18.4, 20.7, 23. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAJCCAYAAAAyQlr2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7af9f53c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(max(lengths))\n",
    "plt.hist(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_godin_word_embedding(path):\n",
    "    print(\"Loading Goding model.\")\n",
    "    return godin_embedding.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_google_word2vec(file_name):\n",
    "    print(\"Loading google news word2vec\")\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_GloVe_embedding(file_name):\n",
    "    print('Loading GloVe word vectors.')\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_fast_text_model(sentences):\n",
    "    try:\n",
    "        m = fasttext.load_model('fast_text_model.bin')\n",
    "        print(\"trained model loaded\")\n",
    "        return m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_embedding_matrix(model,dim):\n",
    "    #dim = 300 for google word2vec\n",
    "    #dim = 400 for godin\n",
    "    #dim = 100 for fast text\n",
    "    embedding_matrix = np.zeros((vocab_size,dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 25\n",
      "Vocabulary size: 2683\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(trainX)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % max_len)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "trainX = encode_text(tokenizer, trainX, max_len)\n",
    "testX = encode_text(tokenizer, testX, max_len)\n",
    "# trainY = to_categorical(trainY,num_classes=number_of_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google news word2vec\n"
     ]
    }
   ],
   "source": [
    "# glove_model = load_GloVe_embedding('word_embeddings/glove.6B.300d.txt')\n",
    "# fast_text_model = load_fast_text_model(back_up_for_fasttext)\n",
    "# godin_model = load_godin_word_embedding(\"word_embeddings/word2vec_twitter_model.bin\")\n",
    "word2vec_model= load_google_word2vec('../word_embeddings/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding_matrix_glove = get_GloVe_embedding_matrix(glove_model)\n",
    "embedding_matrix_word2vec = get_word_embedding_matrix(word2vec_model,300)\n",
    "# embedding_matrix_fast_text = get_word_embedding_matrix(fast_text_model,100)\n",
    "# embedding_matrix_godin = get_word_embedding_matrix(godin_model,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',name='learning_rate')\n",
    "para_dropout = Categorical(categories=[0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],name = 'dropout')\n",
    "# para_em = Categorical(categories=[embedding_matrix_godin,embedding_matrix_word2vec],name='em')\n",
    "para_em = Categorical(categories=['embedding_matrix_word2vec'],name='em')\n",
    "para_em_trainable_flag = Categorical(categories=[True,False],name='em_trainable_flag')\n",
    "para_batch_size = Categorical(categories=[8,16,32,64],name='batch_size')\n",
    "para_epoch = Categorical(categories=[5,10,15,20],name='epoch')\n",
    "para_n_hidden_layers = Integer(low=1,high=5,name = 'n_hidden_layers')\n",
    "\n",
    "para_units_out = Categorical(categories=[64,128,256,512], name='units_out')\n",
    "\n",
    "para_dropout_cnn_lstm = Categorical(categories=[0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],name = 'dropout')\n",
    "\n",
    "para_n_dense = Categorical(categories=[100,200,300,400], name='n_dense')\n",
    "# para_n_filters = Categorical(categories=[100,200,300],name='n_filters')\n",
    "# para_filter_size = Integer(low=1,high=6,name = 'filter_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters_cnn = [para_learning_rate,para_dropout,para_n_dense,para_n_filters,para_filter_size,para_em,para_em_trainable_flag,para_batch_size,para_epoch,para_n_hidden_layers]\n",
    "parameters_lstm = [para_learning_rate,para_dropout,para_units_out,para_em,para_em_trainable_flag,para_batch_size,para_epoch,para_n_hidden_layers,para_n_dense]\n",
    "# parameters_cnn_lstm = [para_learning_rate,para_dropout,para_dropout_cnn_lstm,para_units_out,para_n_filters,para_filter_size,para_em,para_em_trainable_flag,para_batch_size,para_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default_parameters_cnn = [0.0001,0.6,100,200,1,'embedding_matrix_word2vec',True,8,20]\n",
    "default_parameters_lstm = [0.001,0.5,128,'embedding_matrix_word2vec',True,32,10,2,100]\n",
    "# default_parameters_cnn_lstm = [0.001,0.2,0.2,128,100,5,embedding_matrix_word2vec,True,32,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = 1\n",
    "record = {}\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##this will change based on the model\n",
    "@use_named_args(dimensions=parameters_lstm)\n",
    "def fitness(learning_rate,dropout,units_out,em,em_trainable_flag,batch_size,epoch,n_hidden_layers,n_dense):\n",
    "    global key\n",
    "    global record\n",
    "    global number_of_classes\n",
    "    print('-----------------------------combination no={0}------------------'.format(key))\n",
    "    parameters = {\n",
    "            \"dropout\": dropout,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"units_out\": units_out,\n",
    "            \"em\": em,\n",
    "            \"em_trainable_flag\":em_trainable_flag,\n",
    "            \"batch\": batch_size,\n",
    "            \"epoch\": epoch,\n",
    "            \"n_hidden_layers\":int(n_hidden_layers),\n",
    "            \"n_dense\":n_dense\n",
    "        }\n",
    "    \n",
    "    pprint(parameters)\n",
    "    \n",
    "    model = lstm(length=max_len,\n",
    "                 vocab_size=vocab_size,\n",
    "                 learning_rate=parameters['learning_rate'],\n",
    "                 dropout=parameters['dropout'],\n",
    "                 units_out=parameters['units_out'],\n",
    "                 em = eval(parameters['em']),\n",
    "                 number_of_classes = number_of_classes,\n",
    "                 em_trainable_flag = parameters['em_trainable_flag'],\n",
    "                 n_dense = parameters['n_dense'],\n",
    "                 n_hidden_layers=parameters['n_hidden_layers'])\n",
    "\n",
    "    history = model.fit(trainX,trainY,epochs=parameters[\"epoch\"],batch_size=parameters[\"batch\"])\n",
    "    pred = model.predict(testX)\n",
    "    pred_class = [np.argmax(x) for x in pred]\n",
    "    acc = accuracy_score(test_labels,pred_class)\n",
    "    print(acc)\n",
    "    \n",
    "    record[key] = {}\n",
    "    record[key][\"parameter\"] = parameters\n",
    "    record[key][\"acc\"] = acc\n",
    "    \n",
    "    with open(\"results/lstm.json\",'w')as fout:\n",
    "        json.dump(record,fout,indent=4)\n",
    "    \n",
    "    if acc>best_acc:\n",
    "        model.save(\"models/best_lstm.h5\")\n",
    "    \n",
    "    key+=1\n",
    "    \n",
    "    del model\n",
    "    K.clear_session()\n",
    "    \n",
    "    return -acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------combination no=7------------------\n",
      "{'batch': 32,\n",
      " 'dropout': 0.5,\n",
      " 'em': 'embedding_matrix_word2vec',\n",
      " 'em_trainable_flag': True,\n",
      " 'epoch': 10,\n",
      " 'learning_rate': 0.001,\n",
      " 'n_dense': 100,\n",
      " 'n_hidden_layers': 2,\n",
      " 'units_out': 128}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 300)           804900    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,042,549\n",
      "Trainable params: 1,042,549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.6918 - acc: 0.5420\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.6862 - acc: 0.5360\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.6859 - acc: 0.5260\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.6294 - acc: 0.6360\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.5968 - acc: 0.7140\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.4932 - acc: 0.8140\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.4107 - acc: 0.8600\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.3424 - acc: 0.8920\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.2017 - acc: 0.9620\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.2044 - acc: 0.9500\n",
      "0.51\n",
      "-----------------------------combination no=8------------------\n",
      "{'batch': 8,\n",
      " 'dropout': 0.3,\n",
      " 'em': 'embedding_matrix_word2vec',\n",
      " 'em_trainable_flag': True,\n",
      " 'epoch': 15,\n",
      " 'learning_rate': 0.006877453795307447,\n",
      " 'n_dense': 200,\n",
      " 'n_hidden_layers': 4,\n",
      " 'units_out': 256}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 300)           804900    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               51400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,477,019\n",
      "Trainable params: 1,477,019\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.7272 - acc: 0.5160\n",
      "Epoch 2/15\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.7075 - acc: 0.5120\n",
      "Epoch 3/15\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.6961 - acc: 0.5060\n",
      "Epoch 4/15\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.6981 - acc: 0.5360\n",
      "Epoch 5/15\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.6940 - acc: 0.5260\n",
      "Epoch 6/15\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.6919 - acc: 0.5260\n",
      "Epoch 7/15\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.6922 - acc: 0.5260\n",
      "Epoch 8/15\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.6924 - acc: 0.5260\n",
      "Epoch 9/15\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.6916 - acc: 0.5260\n",
      "Epoch 10/15\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.6937 - acc: 0.5260\n",
      "Epoch 11/15\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.6924 - acc: 0.5260\n",
      "Epoch 12/15\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.6923 - acc: 0.5260\n",
      "Epoch 13/15\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.6924 - acc: 0.5260\n",
      "Epoch 14/15\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.6920 - acc: 0.5260\n",
      "Epoch 15/15\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.6922 - acc: 0.5260\n",
      "0.51\n",
      "-----------------------------combination no=9------------------\n",
      "{'batch': 32,\n",
      " 'dropout': 0.9,\n",
      " 'em': 'embedding_matrix_word2vec',\n",
      " 'em_trainable_flag': False,\n",
      " 'epoch': 10,\n",
      " 'learning_rate': 0.002805651440651631,\n",
      " 'n_dense': 200,\n",
      " 'n_hidden_layers': 2,\n",
      " 'units_out': 128}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 300)           804900    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               25800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,070,549\n",
      "Trainable params: 265,649\n",
      "Non-trainable params: 804,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.8864 - acc: 0.4560\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.8932 - acc: 0.4880\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.8239 - acc: 0.5240\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7934 - acc: 0.5160\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7520 - acc: 0.5320\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7631 - acc: 0.5080\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7437 - acc: 0.5280\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7342 - acc: 0.5380\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7406 - acc: 0.5140\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.7491 - acc: 0.4800\n",
      "0.51\n",
      "-----------------------------combination no=10------------------\n",
      "{'batch': 64,\n",
      " 'dropout': 0.7,\n",
      " 'em': 'embedding_matrix_word2vec',\n",
      " 'em_trainable_flag': False,\n",
      " 'epoch': 15,\n",
      " 'learning_rate': 0.0014608391285678966,\n",
      " 'n_dense': 400,\n",
      " 'n_hidden_layers': 1,\n",
      " 'units_out': 256}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 300)           804900    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               102800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 1,478,469\n",
      "Trainable params: 673,569\n",
      "Non-trainable params: 804,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.6934 - acc: 0.4920\n",
      "Epoch 2/15\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 0.6920 - acc: 0.5120\n",
      "Epoch 3/15\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.6941 - acc: 0.5260\n",
      "Epoch 4/15\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.6905 - acc: 0.5180\n",
      "Epoch 5/15\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.6983 - acc: 0.5080\n",
      "Epoch 6/15\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 0.6892 - acc: 0.5460\n",
      "Epoch 7/15\n",
      "320/500 [==================>...........] - ETA: 0s - loss: 0.6899 - acc: 0.5437"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-b55186356092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0macq_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'EI'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                             x0=default_parameters_lstm)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-54142f65a7d2>\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(learning_rate, dropout, units_out, em, em_trainable_flag, batch_size, epoch, n_hidden_layers, n_dense)\u001b[0m\n\u001b[1;32m     31\u001b[0m                  n_hidden_layers=parameters['n_hidden_layers'])\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mpred_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters_lstm,\n",
    "                            acq_func='EI',\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters_cnn = {\n",
    "#             \"n_dense\": 250,\n",
    "#             \"dropout\": 0.2,\n",
    "#             \"learning_rate\": 0.001,\n",
    "#             \"n_filters\": 100,\n",
    "#             \"filter_size\": 5,\n",
    "#             \"em\": embedding_matrix_word2vec,\n",
    "#             \"em_trainable_flag\":True,\n",
    "#             \"batch\": 32,\n",
    "#             \"epoch\": 5\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters_lstm_or_gru = {\n",
    "#             \"dropout\": 0.5,\n",
    "#             \"learning_rate\": 0.001,\n",
    "#             \"units_out\": 64,\n",
    "#             \"em\": embedding_matrix_word2vec,\n",
    "#             \"em_trainable_flag\":True,\n",
    "#             \"batch\": 32,\n",
    "#             \"epoch\": 4\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters_cnn_lstm_or_gru = {\n",
    "#             \"n_filters\": 100,\n",
    "#             \"filter_size\": 5,\n",
    "#             \"conv_dropout\": 0.5,\n",
    "#             \"l_or_g_dropout\":0.5,\n",
    "#             \"learning_rate\": 0.001,\n",
    "#             \"units_out\": 64,\n",
    "#             \"em\": embedding_matrix_word2vec,\n",
    "#             \"em_trainable_flag\":True,\n",
    "#             \"batch\": 32,\n",
    "#             \"epoch\": 4\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = cnn(length=max_len,\n",
    "#             vocab_size=vocab_size,\n",
    "#             n_dense=parameters_cnn['n_dense'],\n",
    "#             dropout=parameters_cnn['dropout'],\n",
    "#             learning_rate=parameters_cnn['learning_rate'],\n",
    "#             n_filters=parameters_cnn['n_filters'],\n",
    "#             filter_size=parameters_cnn['filter_size'],\n",
    "#             em = parameters_cnn['em'],\n",
    "#             free_em_dim=parameters_cnn['free_em_dim'],\n",
    "#             number_of_classes=number_of_classes,\n",
    "#             em_trainable_flag=parameters_cnn['em_trainable_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = bi_gru(length=max_len,\n",
    "#              vocab_size=vocab_size,\n",
    "#              learning_rate=parameters_lstm_or_gru['learning_rate'],\n",
    "#              dropout=parameters_lstm_or_gru['dropout'],\n",
    "#              units_out=parameters_lstm_or_gru['units_out'],\n",
    "#              em=parameters_lstm_or_gru['em'],\n",
    "#              number_of_classes=number_of_classes,\n",
    "#              em_trainable_flag=parameters_lstm_or_gru['em_trainable_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = cnn_lstm(length=max_len,\n",
    "#                     vocab_size=vocab_size,\n",
    "#                     n_filters=parameters_cnn_lstm_or_gru['n_filters'],\n",
    "#                     filter_size=parameters_cnn_lstm_or_gru['filter_size'],\n",
    "#                     em=parameters_cnn_lstm_or_gru['em'],\n",
    "#                     number_of_classes=number_of_classes,\n",
    "#                     em_trainable_flag=parameters_cnn_lstm_or_gru['em_trainable_flag'],\n",
    "#                     learning_rate=parameters_cnn_lstm_or_gru['learning_rate'],\n",
    "#                     conv_dropout=parameters_cnn_lstm_or_gru['conv_dropout'],\n",
    "#                     l_or_g_dropout=parameters_cnn_lstm_or_gru['l_or_g_dropout'],\n",
    "#                     units_out=parameters_cnn_lstm_or_gru['units_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
