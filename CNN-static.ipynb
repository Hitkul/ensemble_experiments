{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "from gensim.models import KeyedVectors\n",
    "import word2vecReader as godin_embedding\n",
    "import fasttext\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,Embedding\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from random import uniform,choice\n",
    "from os import remove\n",
    "import re\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_file(filename):\n",
    "    with open(filename,'r') as fin:\n",
    "        lines = fin.readlines()\n",
    "    \n",
    "    label = [int(x.split()[0]) for x in lines]\n",
    "    sentence = [' '.join(x.split()[1:]) for x in lines]\n",
    "    return label,sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train,sentence_train = load_data_from_file('dataset/sst2/stsa.binary.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_dev,sentence_dev = load_data_from_file('dataset/sst2/stsa.binary.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = sentence_train+sentence_dev\n",
    "train_labels = label_train+label_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7792, 7792)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences),len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_classes = len(set(train_labels))\n",
    "number_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3738\n",
      "4054\n"
     ]
    }
   ],
   "source": [
    "for ele in list(set(train_labels)):\n",
    "    print(train_labels.count(ele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #for PYTHON 2.7\n",
    "    #tokens = [w.translate(None, string.punctuation) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train data\n"
     ]
    }
   ],
   "source": [
    "print(\"cleaning train data\")\n",
    "trainX = [clean_sentence(s) for s in train_sentences]\n",
    "trainY = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = [len(s.split()) for s in trainX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAAIMCAYAAAAKDkGtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff501c37e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length)\n",
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading GloVe embedding\n",
    "def load_GloVe_embedding(file_name):\n",
    "    print('Loading GloVe word vectors.')\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "def get_GloVe_embedding_matrix(embeddings_index):\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fast text word embedding\n",
    "def load_fast_text_model(sentences):\n",
    "    try:\n",
    "        m = fasttext.load_model('word_embeddings/fast_text_model.bin')\n",
    "        print(\"trained model loaded\")\n",
    "        return m\n",
    "    except:\n",
    "        print(\"traning new model\")\n",
    "        with open('temp_file.txt','w') as temp_file:\n",
    "            for sentence in sentences:\n",
    "                temp_file.write(sentence)\n",
    "        m = fasttext.cbow('temp_file.txt','word_embeddings/fast_text_model')\n",
    "        remove('temp_file.txt')\n",
    "        print('model trained')\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fast_text_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading godin word embedding\n",
    "def load_godin_word_embedding(path):\n",
    "    print(\"Loading Goding model.\")\n",
    "    return godin_embedding.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_godin_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,400))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading Google Word2Vec\n",
    "def load_google_word2vec(file_name):\n",
    "    print(\"Loading google news word2vec\")\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(length,vocab_size,n_dense,dropout,learning_rate,n_filters,filter_size,em,free_em_dim,number_of_classes):\n",
    "    inputs = Input(shape=(length,))\n",
    "    if em == 'free':\n",
    "        embedding = Embedding(vocab_size, free_em_dim)(inputs)\n",
    "    else:\n",
    "        embedding = Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=length,trainable = False)(inputs)\n",
    "    \n",
    "    conv = Conv1D(filters=n_filters, kernel_size=filter_size, activation='relu')(embedding)\n",
    "    drop = Dropout(dropout)(conv)\n",
    "    pool = MaxPooling1D(pool_size=2)(drop)\n",
    "    flat = Flatten()(pool)\n",
    "    # interpretation\n",
    "    dense = Dense(n_dense, activation='relu')(flat)\n",
    "    outputs = Dense(number_of_classes, activation='sigmoid')(dense)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # compile\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 25\n",
      "Vocabulary size: 15312\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(trainX)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % max_len)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "trainX = encode_text(tokenizer, trainX, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google news word2vec\n"
     ]
    }
   ],
   "source": [
    "# glove_model = load_GloVe_embedding('word_embeddings/glove.6B.300d.txt')\n",
    "# fast_text_model = load_fast_text_model(train_sentences)\n",
    "# godin_model = load_godin_word_embedding(\"word_embeddings/word2vec_twitter_model.bin\")\n",
    "word2vec_model= load_google_word2vec('word_embeddings/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding_matrix_glove = get_GloVe_embedding_matrix(glove_model)\n",
    "embedding_matrix_word2vec = get_word2vec_embedding_matrix(word2vec_model)\n",
    "# embedding_matrix_fast_text = get_fast_text_matrix(fast_text_model)\n",
    "# embedding_matrix_godin = get_godin_embedding_matrix(godin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',name='learning_rate')\n",
    "\n",
    "para_dropout = Real(low=0.4, high=0.9,name = 'dropout')\n",
    "\n",
    "para_n_dense = Categorical(categories=[100,200,300,400], name='n_dense')\n",
    "\n",
    "para_n_filters = Categorical(categories=[10,32,64,100,200],name='n_filters')\n",
    "\n",
    "para_filter_size = Integer(low=1,high=8,name = 'filter_size')\n",
    "para_em = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em')\n",
    "\n",
    "para_free_em_dim = Categorical(categories=[100,300,400],name='free_em_dim')\n",
    "\n",
    "para_batch_size = Categorical(categories=[8,16,32,64],name='batch_size')\n",
    "\n",
    "para_epoch = Categorical(categories=[10,20,50,100,150],name='epoch')\n",
    "\n",
    "\n",
    "parameters = [para_learning_rate,para_dropout,para_n_dense,para_n_filters,para_filter_size,para_em,para_free_em_dim,para_batch_size,para_epoch]\n",
    "\n",
    "default_parameters = [0.001,0.5777195655120914,100,32,3,'embedding_matrix_word2vec',100,64,10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = 1\n",
    "record = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=parameters)\n",
    "def fitness(learning_rate,dropout,n_dense,n_filters,filter_size,em,free_em_dim,batch_size,epoch):\n",
    "    global key\n",
    "    global record\n",
    "    global number_of_classes\n",
    "    print('-----------------------------combination no={0}------------------'.format(key))\n",
    "    parameters = {\n",
    "            \"n_dense\": n_dense,\n",
    "            \"dropout\": dropout,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"n_filters\": n_filters,\n",
    "            \"filter_size\": filter_size,\n",
    "            \"em\": em,\n",
    "            \"free_em_dim\": free_em_dim,\n",
    "            \"batch\": batch_size,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "    \n",
    "    pprint(parameters)\n",
    "    \n",
    "    itr = 1\n",
    "    acc_record = []\n",
    "    itr_record = {}\n",
    "    for train,test in kfold.split(trainX,trainY):\n",
    "        print(\"k fold validation itr == {0}\".format(itr))\n",
    "        X = trainX[train]\n",
    "        Y = to_categorical(trainY[train],num_classes=number_of_classes)\n",
    "        X_ = trainX[test]\n",
    "        Y_ = to_categorical(trainY[test],num_classes=number_of_classes)\n",
    "        model = define_model(length = max_len,\n",
    "                             vocab_size=vocab_size,\n",
    "                             n_dense = parameters[\"n_dense\"],\n",
    "                             dropout = parameters[\"dropout\"],\n",
    "                             learning_rate = parameters[\"learning_rate\"],\n",
    "                             n_filters = parameters[\"n_filters\"],\n",
    "                             filter_size = parameters[\"filter_size\"],\n",
    "                             em = parameters[\"em\"],\n",
    "                             free_em_dim = parameters[\"free_em_dim\"],\n",
    "                             number_of_classes = number_of_classes)\n",
    "        history = model.fit(X,Y,validation_data = [X_,Y_],epochs=parameters[\"epoch\"],batch_size=parameters[\"batch\"])\n",
    "        acc = history.history['val_acc'][-1]\n",
    "        print(acc)\n",
    "        acc_record.append(acc)\n",
    "        itr_record[itr] = {}\n",
    "        itr_record[itr][\"acc\"] = acc\n",
    "        model.save('models/'+str(key)+'_'+str(itr)+'.h5')\n",
    "        itr+=1\n",
    "    record[key] = {}\n",
    "    record[key][\"parameter\"] = parameters\n",
    "    mean_acc = np.mean(acc_record)\n",
    "    record[key][\"mean_acc\"] = mean_acc\n",
    "    record[key][\"itr_record\"] = itr_record\n",
    "    with open(\"models/record.json\",'w')as fout:\n",
    "        json.dump(record,fout,indent=4)\n",
    "    key+=1\n",
    "    \n",
    "    del model\n",
    "    K.clear_session()\n",
    "    \n",
    "    return -mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------combination no=1------------------\n",
      "{'batch': 64,\n",
      " 'dropout': 0.5777195655120914,\n",
      " 'em': 'embedding_matrix_word2vec',\n",
      " 'epoch': 10,\n",
      " 'filter_size': 3,\n",
      " 'free_em_dim': 100,\n",
      " 'learning_rate': 0.001,\n",
      " 'n_dense': 100,\n",
      " 'n_filters': 32}\n",
      "k fold validation itr == 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 25, 300)           4593600   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 23, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               35300     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 4,657,934\n",
      "Trainable params: 64,334\n",
      "Non-trainable params: 4,593,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7012 samples, validate on 780 samples\n",
      "Epoch 1/10\n",
      "7012/7012 [==============================] - 2s 282us/step - loss: 0.5904 - acc: 0.6767 - val_loss: 0.4908 - val_acc: 0.7821\n",
      "Epoch 2/10\n",
      "7012/7012 [==============================] - 2s 235us/step - loss: 0.4518 - acc: 0.7884 - val_loss: 0.4636 - val_acc: 0.7891\n",
      "Epoch 3/10\n",
      "7012/7012 [==============================] - 2s 233us/step - loss: 0.4076 - acc: 0.8150 - val_loss: 0.4556 - val_acc: 0.7865\n",
      "Epoch 4/10\n",
      "7012/7012 [==============================] - 2s 241us/step - loss: 0.3773 - acc: 0.8322 - val_loss: 0.4531 - val_acc: 0.7917\n",
      "Epoch 5/10\n",
      "7012/7012 [==============================] - 2s 248us/step - loss: 0.3511 - acc: 0.8453 - val_loss: 0.4662 - val_acc: 0.7795\n",
      "Epoch 6/10\n",
      "7012/7012 [==============================] - 2s 233us/step - loss: 0.3308 - acc: 0.8518 - val_loss: 0.4662 - val_acc: 0.7962\n",
      "Epoch 7/10\n",
      "7012/7012 [==============================] - 2s 235us/step - loss: 0.3085 - acc: 0.8696 - val_loss: 0.4753 - val_acc: 0.7949\n",
      "Epoch 8/10\n",
      "7012/7012 [==============================] - 2s 233us/step - loss: 0.2851 - acc: 0.8779 - val_loss: 0.4826 - val_acc: 0.7840\n",
      "Epoch 9/10\n",
      "7012/7012 [==============================] - 2s 234us/step - loss: 0.2738 - acc: 0.8847 - val_loss: 0.4981 - val_acc: 0.7801\n",
      "Epoch 10/10\n",
      "7012/7012 [==============================] - 2s 241us/step - loss: 0.2541 - acc: 0.8920 - val_loss: 0.5088 - val_acc: 0.7769\n",
      "0.7769230769230769\n",
      "k fold validation itr == 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 25, 300)           4593600   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 23, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               35300     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 4,657,934\n",
      "Trainable params: 64,334\n",
      "Non-trainable params: 4,593,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7012 samples, validate on 780 samples\n",
      "Epoch 1/10\n",
      "7012/7012 [==============================] - 2s 261us/step - loss: 0.6058 - acc: 0.6606 - val_loss: 0.4904 - val_acc: 0.7885\n",
      "Epoch 2/10\n",
      "7012/7012 [==============================] - 2s 242us/step - loss: 0.4576 - acc: 0.7889 - val_loss: 0.4639 - val_acc: 0.7744\n",
      "Epoch 3/10\n",
      "7012/7012 [==============================] - 2s 240us/step - loss: 0.4074 - acc: 0.8148 - val_loss: 0.4369 - val_acc: 0.8051\n",
      "Epoch 4/10\n",
      "7012/7012 [==============================] - 2s 242us/step - loss: 0.3887 - acc: 0.8296 - val_loss: 0.4342 - val_acc: 0.8051\n",
      "Epoch 5/10\n",
      "7012/7012 [==============================] - 2s 237us/step - loss: 0.3568 - acc: 0.8443 - val_loss: 0.4397 - val_acc: 0.7994\n",
      "Epoch 6/10\n",
      "7012/7012 [==============================] - 2s 233us/step - loss: 0.3316 - acc: 0.8562 - val_loss: 0.4426 - val_acc: 0.7949\n",
      "Epoch 7/10\n",
      "7012/7012 [==============================] - 2s 233us/step - loss: 0.3082 - acc: 0.8665 - val_loss: 0.4527 - val_acc: 0.7904\n",
      "Epoch 8/10\n",
      "7012/7012 [==============================] - 2s 236us/step - loss: 0.2829 - acc: 0.8815 - val_loss: 0.4816 - val_acc: 0.7833\n",
      "Epoch 9/10\n",
      "7012/7012 [==============================] - 2s 237us/step - loss: 0.2680 - acc: 0.8861 - val_loss: 0.4707 - val_acc: 0.7814\n",
      "Epoch 10/10\n",
      "7012/7012 [==============================] - 2s 233us/step - loss: 0.2502 - acc: 0.8950 - val_loss: 0.4897 - val_acc: 0.7917\n",
      "0.7916666666666666\n",
      "k fold validation itr == 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 25, 300)           4593600   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 23, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               35300     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 4,657,934\n",
      "Trainable params: 64,334\n",
      "Non-trainable params: 4,593,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7012 samples, validate on 780 samples\n",
      "Epoch 1/10\n",
      "7012/7012 [==============================] - 2s 264us/step - loss: 0.5871 - acc: 0.6823 - val_loss: 0.4796 - val_acc: 0.7968\n",
      "Epoch 2/10\n",
      "7012/7012 [==============================] - 2s 272us/step - loss: 0.4513 - acc: 0.7899 - val_loss: 0.4463 - val_acc: 0.8115\n",
      "Epoch 3/10\n",
      "7012/7012 [==============================] - 2s 278us/step - loss: 0.4121 - acc: 0.8167 - val_loss: 0.4278 - val_acc: 0.8218\n",
      "Epoch 4/10\n",
      "7012/7012 [==============================] - 2s 249us/step - loss: 0.3877 - acc: 0.8263 - val_loss: 0.4242 - val_acc: 0.8205\n",
      "Epoch 5/10\n",
      "7012/7012 [==============================] - 2s 246us/step - loss: 0.3618 - acc: 0.8451 - val_loss: 0.4355 - val_acc: 0.8109\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7012/7012 [==============================] - 2s 223us/step - loss: 0.3284 - acc: 0.8607 - val_loss: 0.4335 - val_acc: 0.8038\n",
      "Epoch 7/10\n",
      "7012/7012 [==============================] - 2s 224us/step - loss: 0.3151 - acc: 0.8613 - val_loss: 0.4346 - val_acc: 0.8058\n",
      "Epoch 8/10\n",
      "7012/7012 [==============================] - 2s 223us/step - loss: 0.2863 - acc: 0.8791 - val_loss: 0.4389 - val_acc: 0.8026\n",
      "Epoch 9/10\n",
      "7012/7012 [==============================] - 2s 223us/step - loss: 0.2729 - acc: 0.8860 - val_loss: 0.4663 - val_acc: 0.8000\n",
      "Epoch 10/10\n",
      "7012/7012 [==============================] - 2s 223us/step - loss: 0.2598 - acc: 0.8893 - val_loss: 0.4726 - val_acc: 0.7891\n",
      "0.7891025641025641\n",
      "k fold validation itr == 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 25, 300)           4593600   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 23, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               35300     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 4,657,934\n",
      "Trainable params: 64,334\n",
      "Non-trainable params: 4,593,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7012 samples, validate on 780 samples\n",
      "Epoch 1/10\n",
      "7012/7012 [==============================] - 2s 256us/step - loss: 0.5949 - acc: 0.6761 - val_loss: 0.4864 - val_acc: 0.7827\n",
      "Epoch 2/10\n",
      "7012/7012 [==============================] - 2s 227us/step - loss: 0.4590 - acc: 0.7882 - val_loss: 0.4392 - val_acc: 0.7910\n",
      "Epoch 3/10\n",
      "7012/7012 [==============================] - 2s 225us/step - loss: 0.4177 - acc: 0.8115 - val_loss: 0.4230 - val_acc: 0.8026\n",
      "Epoch 4/10\n",
      "7012/7012 [==============================] - 2s 226us/step - loss: 0.3908 - acc: 0.8232 - val_loss: 0.4134 - val_acc: 0.8083\n",
      "Epoch 5/10\n",
      "7012/7012 [==============================] - 2s 224us/step - loss: 0.3648 - acc: 0.8371 - val_loss: 0.4140 - val_acc: 0.8077\n",
      "Epoch 6/10\n",
      "7012/7012 [==============================] - 2s 229us/step - loss: 0.3258 - acc: 0.8614 - val_loss: 0.4153 - val_acc: 0.8083\n",
      "Epoch 7/10\n",
      "7012/7012 [==============================] - 2s 229us/step - loss: 0.3154 - acc: 0.8615 - val_loss: 0.4245 - val_acc: 0.8083\n",
      "Epoch 8/10\n",
      "7012/7012 [==============================] - 2s 254us/step - loss: 0.3025 - acc: 0.8736 - val_loss: 0.4234 - val_acc: 0.7910\n",
      "Epoch 9/10\n",
      "7012/7012 [==============================] - 2s 270us/step - loss: 0.2694 - acc: 0.8895 - val_loss: 0.4434 - val_acc: 0.7974\n",
      "Epoch 10/10\n",
      "7012/7012 [==============================] - 2s 264us/step - loss: 0.2615 - acc: 0.8873 - val_loss: 0.4397 - val_acc: 0.7897\n",
      "0.7897435897435897\n",
      "k fold validation itr == 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 25, 300)           4593600   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 23, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               35300     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 4,657,934\n",
      "Trainable params: 64,334\n",
      "Non-trainable params: 4,593,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7013 samples, validate on 779 samples\n",
      "Epoch 1/10\n",
      "7013/7013 [==============================] - 2s 314us/step - loss: 0.6114 - acc: 0.6524 - val_loss: 0.4864 - val_acc: 0.7689\n",
      "Epoch 2/10\n",
      "7013/7013 [==============================] - 2s 274us/step - loss: 0.4518 - acc: 0.7927 - val_loss: 0.4478 - val_acc: 0.7901\n",
      "Epoch 3/10\n",
      "7013/7013 [==============================] - 2s 268us/step - loss: 0.4130 - acc: 0.8087 - val_loss: 0.4442 - val_acc: 0.7837\n",
      "Epoch 4/10\n",
      "7013/7013 [==============================] - 2s 293us/step - loss: 0.3843 - acc: 0.8302 - val_loss: 0.4289 - val_acc: 0.8010\n",
      "Epoch 5/10\n",
      "7013/7013 [==============================] - 2s 303us/step - loss: 0.3496 - acc: 0.8499 - val_loss: 0.4308 - val_acc: 0.7940\n",
      "Epoch 6/10\n",
      "7013/7013 [==============================] - 2s 246us/step - loss: 0.3280 - acc: 0.8548 - val_loss: 0.4427 - val_acc: 0.7908\n",
      "Epoch 7/10\n",
      "7013/7013 [==============================] - 2s 255us/step - loss: 0.3044 - acc: 0.8692 - val_loss: 0.4410 - val_acc: 0.7824\n",
      "Epoch 8/10\n",
      "7013/7013 [==============================] - 2s 247us/step - loss: 0.2840 - acc: 0.8829 - val_loss: 0.4306 - val_acc: 0.7959\n",
      "Epoch 9/10\n",
      "7013/7013 [==============================] - 2s 241us/step - loss: 0.2569 - acc: 0.8921 - val_loss: 0.4506 - val_acc: 0.7837\n",
      "Epoch 10/10\n",
      "7013/7013 [==============================] - 2s 242us/step - loss: 0.2500 - acc: 0.8973 - val_loss: 0.4566 - val_acc: 0.7985\n",
      "0.7984595633899753\n",
      "k fold validation itr == 6\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 25, 300)           4593600   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 23, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               35300     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 4,657,934\n",
      "Trainable params: 64,334\n",
      "Non-trainable params: 4,593,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7013 samples, validate on 779 samples\n",
      "Epoch 1/10\n",
      "7013/7013 [==============================] - 2s 289us/step - loss: 0.6031 - acc: 0.6628 - val_loss: 0.4641 - val_acc: 0.7991\n",
      "Epoch 2/10\n",
      "7013/7013 [==============================] - 2s 248us/step - loss: 0.4646 - acc: 0.7855 - val_loss: 0.4263 - val_acc: 0.8113\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7013/7013 [==============================] - 2s 227us/step - loss: 0.4230 - acc: 0.8084 - val_loss: 0.4056 - val_acc: 0.8049\n",
      "Epoch 4/10\n",
      "7013/7013 [==============================] - 2s 223us/step - loss: 0.3924 - acc: 0.8262 - val_loss: 0.3967 - val_acc: 0.8216\n",
      "Epoch 5/10\n",
      "7013/7013 [==============================] - 2s 225us/step - loss: 0.3631 - acc: 0.8428 - val_loss: 0.3842 - val_acc: 0.8248\n",
      "Epoch 6/10\n",
      "7013/7013 [==============================] - 2s 225us/step - loss: 0.3336 - acc: 0.8573 - val_loss: 0.3910 - val_acc: 0.8209\n",
      "Epoch 7/10\n",
      "7013/7013 [==============================] - 2s 226us/step - loss: 0.3189 - acc: 0.8619 - val_loss: 0.3943 - val_acc: 0.8196\n",
      "Epoch 8/10\n",
      "7013/7013 [==============================] - 2s 224us/step - loss: 0.2993 - acc: 0.8727 - val_loss: 0.4296 - val_acc: 0.7914\n",
      "Epoch 9/10\n",
      "7013/7013 [==============================] - 2s 226us/step - loss: 0.2749 - acc: 0.8838 - val_loss: 0.4050 - val_acc: 0.8074\n",
      "Epoch 10/10\n",
      "7013/7013 [==============================] - 2s 225us/step - loss: 0.2614 - acc: 0.8907 - val_loss: 0.4200 - val_acc: 0.7940\n",
      "0.7939666242593366\n",
      "k fold validation itr == 7\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 25, 300)           4593600   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 23, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               35300     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 4,657,934\n",
      "Trainable params: 64,334\n",
      "Non-trainable params: 4,593,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7013 samples, validate on 779 samples\n",
      "Epoch 1/10\n",
      "7013/7013 [==============================] - 2s 269us/step - loss: 0.6003 - acc: 0.6633 - val_loss: 0.4689 - val_acc: 0.7843\n",
      "Epoch 2/10\n",
      "7013/7013 [==============================] - 2s 228us/step - loss: 0.4613 - acc: 0.7907 - val_loss: 0.4276 - val_acc: 0.7946\n",
      "Epoch 3/10\n",
      "7013/7013 [==============================] - 2s 235us/step - loss: 0.4233 - acc: 0.8106 - val_loss: 0.4140 - val_acc: 0.7991\n",
      "Epoch 4/10\n",
      "7013/7013 [==============================] - 2s 261us/step - loss: 0.3960 - acc: 0.8209 - val_loss: 0.3980 - val_acc: 0.8023\n",
      "Epoch 5/10\n",
      "7013/7013 [==============================] - 2s 266us/step - loss: 0.3696 - acc: 0.8417 - val_loss: 0.3956 - val_acc: 0.8030\n",
      "Epoch 6/10\n",
      "7013/7013 [==============================] - 2s 284us/step - loss: 0.3340 - acc: 0.8517 - val_loss: 0.3997 - val_acc: 0.7959\n",
      "Epoch 7/10\n",
      "7013/7013 [==============================] - 2s 260us/step - loss: 0.3132 - acc: 0.8617 - val_loss: 0.4018 - val_acc: 0.8010\n",
      "Epoch 8/10\n",
      "7013/7013 [==============================] - 2s 251us/step - loss: 0.2887 - acc: 0.8783 - val_loss: 0.4010 - val_acc: 0.7965\n",
      "Epoch 9/10\n",
      "7013/7013 [==============================] - 2s 258us/step - loss: 0.2729 - acc: 0.8842 - val_loss: 0.4086 - val_acc: 0.8036\n",
      "Epoch 10/10\n",
      "7013/7013 [==============================] - 2s 282us/step - loss: 0.2565 - acc: 0.8947 - val_loss: 0.4099 - val_acc: 0.7972\n",
      "0.7971758664189927\n",
      "k fold validation itr == 8\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 25, 300)           4593600   \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 23, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               35300     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 4,657,934\n",
      "Trainable params: 64,334\n",
      "Non-trainable params: 4,593,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7013 samples, validate on 779 samples\n",
      "Epoch 1/10\n",
      "7013/7013 [==============================] - 2s 325us/step - loss: 0.6010 - acc: 0.6626 - val_loss: 0.4852 - val_acc: 0.7927\n",
      "Epoch 2/10\n",
      "7013/7013 [==============================] - 2s 272us/step - loss: 0.4567 - acc: 0.7886 - val_loss: 0.4501 - val_acc: 0.7991\n",
      "Epoch 3/10\n",
      "7013/7013 [==============================] - 2s 245us/step - loss: 0.4138 - acc: 0.8084 - val_loss: 0.4275 - val_acc: 0.8068\n",
      "Epoch 4/10\n",
      "7013/7013 [==============================] - 2s 257us/step - loss: 0.3878 - acc: 0.8274 - val_loss: 0.4128 - val_acc: 0.8074\n",
      "Epoch 5/10\n",
      "7013/7013 [==============================] - 2s 268us/step - loss: 0.3565 - acc: 0.8404 - val_loss: 0.4074 - val_acc: 0.8081\n",
      "Epoch 6/10\n",
      "7013/7013 [==============================] - 2s 253us/step - loss: 0.3409 - acc: 0.8527 - val_loss: 0.4418 - val_acc: 0.8004\n",
      "Epoch 7/10\n",
      "7013/7013 [==============================] - 2s 246us/step - loss: 0.3144 - acc: 0.8643 - val_loss: 0.4253 - val_acc: 0.8030\n",
      "Epoch 8/10\n",
      "7013/7013 [==============================] - 2s 247us/step - loss: 0.2940 - acc: 0.8722 - val_loss: 0.4418 - val_acc: 0.8030\n",
      "Epoch 9/10\n",
      "7013/7013 [==============================] - 2s 248us/step - loss: 0.2761 - acc: 0.8864 - val_loss: 0.4436 - val_acc: 0.8030\n",
      "Epoch 10/10\n",
      "7013/7013 [==============================] - 2s 248us/step - loss: 0.2533 - acc: 0.8963 - val_loss: 0.4554 - val_acc: 0.7946\n",
      "0.7946084726300564\n",
      "k fold validation itr == 9\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, 25, 300)           4593600   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 23, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               35300     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 4,657,934\n",
      "Trainable params: 64,334\n",
      "Non-trainable params: 4,593,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7014 samples, validate on 778 samples\n",
      "Epoch 1/10\n",
      "7014/7014 [==============================] - 2s 277us/step - loss: 0.5877 - acc: 0.6764 - val_loss: 0.5327 - val_acc: 0.7359\n",
      "Epoch 2/10\n",
      "7014/7014 [==============================] - 2s 224us/step - loss: 0.4533 - acc: 0.7924 - val_loss: 0.4873 - val_acc: 0.7603\n",
      "Epoch 3/10\n",
      "7014/7014 [==============================] - 2s 224us/step - loss: 0.4160 - acc: 0.8074 - val_loss: 0.4666 - val_acc: 0.7905\n",
      "Epoch 4/10\n",
      "7014/7014 [==============================] - 2s 226us/step - loss: 0.3842 - acc: 0.8275 - val_loss: 0.4665 - val_acc: 0.7873\n",
      "Epoch 5/10\n",
      "7014/7014 [==============================] - 2s 225us/step - loss: 0.3575 - acc: 0.8380 - val_loss: 0.4639 - val_acc: 0.7841\n",
      "Epoch 6/10\n",
      "7014/7014 [==============================] - 2s 228us/step - loss: 0.3340 - acc: 0.8579 - val_loss: 0.4637 - val_acc: 0.7860\n",
      "Epoch 7/10\n",
      "7014/7014 [==============================] - 2s 227us/step - loss: 0.3075 - acc: 0.8680 - val_loss: 0.4832 - val_acc: 0.7879\n",
      "Epoch 8/10\n",
      "7014/7014 [==============================] - 2s 223us/step - loss: 0.2842 - acc: 0.8790 - val_loss: 0.4904 - val_acc: 0.7866\n",
      "Epoch 9/10\n",
      "7014/7014 [==============================] - 2s 228us/step - loss: 0.2709 - acc: 0.8849 - val_loss: 0.5009 - val_acc: 0.7731\n",
      "Epoch 10/10\n",
      "7014/7014 [==============================] - 2s 287us/step - loss: 0.2467 - acc: 0.8998 - val_loss: 0.5357 - val_acc: 0.7834\n",
      "0.7834190232894721\n",
      "k fold validation itr == 10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 25, 300)           4593600   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 23, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               35300     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 4,657,934\n",
      "Trainable params: 64,334\n",
      "Non-trainable params: 4,593,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7014 samples, validate on 778 samples\n",
      "Epoch 1/10\n",
      "7014/7014 [==============================] - 2s 312us/step - loss: 0.5970 - acc: 0.6692 - val_loss: 0.4925 - val_acc: 0.7648\n",
      "Epoch 2/10\n",
      "7014/7014 [==============================] - 2s 257us/step - loss: 0.4686 - acc: 0.7856 - val_loss: 0.4497 - val_acc: 0.8014\n",
      "Epoch 3/10\n",
      "7014/7014 [==============================] - 2s 269us/step - loss: 0.4179 - acc: 0.8133 - val_loss: 0.4225 - val_acc: 0.8091\n",
      "Epoch 4/10\n",
      "7014/7014 [==============================] - 2s 253us/step - loss: 0.3891 - acc: 0.8241 - val_loss: 0.4246 - val_acc: 0.8021\n",
      "Epoch 5/10\n",
      "7014/7014 [==============================] - 2s 259us/step - loss: 0.3537 - acc: 0.8475 - val_loss: 0.4098 - val_acc: 0.8188\n",
      "Epoch 6/10\n",
      "7014/7014 [==============================] - 2s 250us/step - loss: 0.3256 - acc: 0.8564 - val_loss: 0.4182 - val_acc: 0.8104\n",
      "Epoch 7/10\n",
      "4800/7014 [===================>..........] - ETA: 0s - loss: 0.3034 - acc: 0.8692"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-f5b44a4ceba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0macq_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'EI'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                             x0=default_parameters)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# User suggested points at which to evaluate the objective first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my0\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mn_calls\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-161-a6425669b411>\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(learning_rate, dropout, n_dense, n_filters, filter_size, em, free_em_dim, batch_size, epoch)\u001b[0m\n\u001b[1;32m     38\u001b[0m                              \u001b[0mfree_em_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"free_em_dim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                              number_of_classes = number_of_classes)\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters,\n",
    "                            acq_func='EI',\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_parameters():\n",
    "#     #range values\n",
    "#     para_n_dense = [100,200,300,400]\n",
    "#     para_n_filters = [100,200,300,400]\n",
    "#     para_filter_size = [1,2,3,4,5,6]\n",
    "# #     para_em = ['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free']\n",
    "#     para_em = ['embedding_matrix_word2vec']\n",
    "#     para_free_em_dim = [100,300,400]\n",
    "#     para_em_trainable_flag = [True,False]\n",
    "#     para_batch_size = [8,16,32,64]\n",
    "# #     para_epoc = [10,30,60,100]\n",
    "#     para_epoc = [10]\n",
    "# #     para_batch_size = [64]\n",
    "#     #selecting_random_value\n",
    "#     parameters = {\"n_dense\": choice(para_n_dense),\n",
    "#             \"dropout\": uniform(0.4, 0.9),\n",
    "#             \"learning_rate\": uniform(0.0001, 0.01),\n",
    "#             \"n_filters\": choice(para_n_filters),\n",
    "#             \"filter_size_c1\": choice(para_filter_size),\n",
    "#             \"filter_size_c2\": choice(para_filter_size),\n",
    "#             \"filter_size_c3\": choice(para_filter_size),\n",
    "#             \"em_c1\": choice(para_em),\n",
    "#             \"em_c2\": choice(para_em),\n",
    "#             \"em_c3\": choice(para_em),\n",
    "#             \"free_em_dim\": choice(para_free_em_dim),\n",
    "#             \"em_trainable_flag_c1\": choice(para_em_trainable_flag),\n",
    "#             \"em_trainable_flag_c2\": choice(para_em_trainable_flag),\n",
    "#             \"em_trainable_flag_c3\": choice(para_em_trainable_flag),\n",
    "#             \"batch\": choice(para_batch_size),\n",
    "#             \"epoch\": choice(para_epoc)\n",
    "#         }\n",
    "#     return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for _ in range(number_of_models):\n",
    "#     itr = 1\n",
    "#     f1_record = []\n",
    "#     p_record = []\n",
    "#     r_record = []\n",
    "#     itr_record = {}\n",
    "#     parameters = get_parameters()\n",
    "#     parameters = {\n",
    "#             \"n_dense\": 400,\n",
    "#             \"dropout\": 0.5777195655120914,\n",
    "#             \"learning_rate\": 0.0071353667446707675,\n",
    "#             \"n_filters\": 100,\n",
    "#             \"filter_size_c1\": 6,\n",
    "#             \"filter_size_c2\": 4,\n",
    "#             \"filter_size_c3\": 4,\n",
    "#             \"em_c1\": \"embedding_matrix_word2vec\",\n",
    "#             \"em_c2\": \"embedding_matrix_word2vec\",\n",
    "#             \"em_c3\": \"embedding_matrix_word2vec\",\n",
    "#             \"free_em_dim\": 400,\n",
    "#             \"em_trainable_flag_c1\": False,\n",
    "#             \"em_trainable_flag_c2\": True,\n",
    "#             \"em_trainable_flag_c3\": False,\n",
    "#             \"batch\": 16,\n",
    "#             \"epoch\": 1\n",
    "#         }\n",
    "#     print(\"model number {0}\".format(key))\n",
    "#     print(parameters)\n",
    "#     for train,test in kfold.split(trainX,trainY):\n",
    "#         print(\"k fold validation itr == {0}\".format(itr))\n",
    "#         X = trainX[train]\n",
    "#         Y = to_categorical(trainY[train],num_classes=3)\n",
    "#         X_ = trainX[test]\n",
    "#         Y_ = list(trainY[test])\n",
    "#         model = define_model(length = max_len,\n",
    "#                              vocab_size=vocab_size,\n",
    "#                              n_dense = parameters[\"n_dense\"],\n",
    "#                              dropout = parameters[\"dropout\"],\n",
    "#                              learning_rate = parameters[\"learning_rate\"],\n",
    "#                              n_filters = parameters[\"n_filters\"],\n",
    "#                              filter_size_c1 = parameters[\"filter_size_c1\"],\n",
    "#                              filter_size_c2 = parameters[\"filter_size_c2\"],\n",
    "#                              filter_size_c3 = parameters[\"filter_size_c3\"],\n",
    "#                              em_c1 = parameters[\"em_c1\"],\n",
    "#                              em_c2 = parameters[\"em_c1\"],\n",
    "#                              em_c3 = parameters[\"em_c1\"],\n",
    "#                              free_em_dim = parameters[\"free_em_dim\"],\n",
    "#                              em_trainable_flag_c1 = parameters[\"em_trainable_flag_c1\"],\n",
    "#                              em_trainable_flag_c2 = parameters[\"em_trainable_flag_c2\"],\n",
    "#                              em_trainable_flag_c3 = parameters[\"em_trainable_flag_c3\"])\n",
    "#         history = model.fit([X,X,X],Y,epochs=parameters[\"epoch\"],batch_size=parameters[\"batch\"])\n",
    "#         pred = model.predict([X_,X_,X_])\n",
    "#         pred_labels = [x.argmax() for x in pred]\n",
    "#         for foo in zip(Y_[:50],pred_labels[:50]):\n",
    "#             print(foo)\n",
    "#         f1 = f1_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "#         p = precision_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "#         r = recall_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "#         print(f1,p,r)\n",
    "#         f1_record.append(f1)\n",
    "#         p_record.append(p)\n",
    "#         r_record.append(r)\n",
    "#         itr_record[itr] = {}\n",
    "#         itr_record[itr][\"f1\"] = f1\n",
    "#         itr_record[itr][\"p\"] = p\n",
    "#         itr_record[itr][\"r\"] = r\n",
    "#         model.save('models/'+str(key)+'_'+str(itr)+'.h5')\n",
    "#         itr+=1\n",
    "#     record[key] = {}\n",
    "#     record[key][\"parameter\"] = parameters\n",
    "#     record[key][\"mean_f1\"] = np.mean(f1_record)\n",
    "#     record[key][\"itr_record\"] = itr_record\n",
    "\n",
    "#     with open(\"models/record.json\",'w')as fout:\n",
    "#         json.dump(record,fout,indent=4)\n",
    "#     key+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
