{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "from gensim.models import KeyedVectors\n",
    "import word2vecReader as godin_embedding\n",
    "import fasttext\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,Embedding\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from random import uniform,choice\n",
    "from os import remove\n",
    "import re\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename,'r') as fin:\n",
    "        return json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_dict = load_data(\"dataset/MR_dataset/final_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(dataset_dict):\n",
    "    review = [dataset_dict[key][\"review\"] for key in dataset_dict.keys()]\n",
    "    polarity = [dataset_dict[key][\"polarity\"] for key in dataset_dict.keys()]\n",
    "    return review, polarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences, train_labels = extract_data(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# review = review[:100]\n",
    "# polarity = polarity[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences),len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.count(0),train_labels.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #for PYTHON 2.7\n",
    "    #tokens = [w.translate(None, string.punctuation) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train data\n"
     ]
    }
   ],
   "source": [
    "print(\"cleaning train data\")\n",
    "trainX = [clean_sentence(s) for s in train_sentences]\n",
    "trainY = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = [len(s.split()) for s in trainX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1380"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAIMCAYAAAAHEDHqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDAAAk/8PFT9y3MlFZ08AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5d7c7beb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length)\n",
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading GloVe embedding\n",
    "def load_GloVe_embedding(file_name):\n",
    "    print('Loading GloVe word vectors.')\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "def get_GloVe_embedding_matrix(embeddings_index):\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fast text word embedding\n",
    "def load_fast_text_model(sentences):\n",
    "    try:\n",
    "        m = fasttext.load_model('word_embeddings/fast_text_model.bin')\n",
    "        print(\"trained model loaded\")\n",
    "        return m\n",
    "    except:\n",
    "        print(\"traning new model\")\n",
    "        with open('temp_file.txt','w') as temp_file:\n",
    "            for sentence in sentences:\n",
    "                temp_file.write(sentence)\n",
    "        m = fasttext.cbow('temp_file.txt','word_embeddings/fast_text_model')\n",
    "        remove('temp_file.txt')\n",
    "        print('model trained')\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fast_text_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading godin word embedding\n",
    "def load_godin_word_embedding(path):\n",
    "    print(\"Loading Goding model.\")\n",
    "    return godin_embedding.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_godin_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,400))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading Google Word2Vec\n",
    "def load_google_word2vec(file_name):\n",
    "    print(\"Loading google news word2vec\")\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(length,vocab_size,n_dense,dropout,learning_rate,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,free_em_dim,em_trainable_flag_c1,em_trainable_flag_c2,em_trainable_flag_c3):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    if em_c1 == 'free':\n",
    "        embedding1 = Embedding(vocab_size, free_em_dim)(inputs1)\n",
    "    else:\n",
    "        embedding1 = Embedding(vocab_size, len(eval(em_c1)[0]), weights = [eval(em_c1)],input_length=length,trainable = em_trainable_flag_c1)(inputs1)\n",
    "    \n",
    "    conv1 = Conv1D(filters=n_filters, kernel_size=filter_size_c1, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(dropout)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    if em_c2 == 'free':\n",
    "        embedding2 = Embedding(vocab_size, free_em_dim)(inputs2)\n",
    "    else:\n",
    "        embedding2 = Embedding(vocab_size, len(eval(em_c2)[0]), weights = [eval(em_c2)],input_length=length,trainable = em_trainable_flag_c2)(inputs2)\n",
    "    conv2 = Conv1D(filters=n_filters, kernel_size=filter_size_c2, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(dropout)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    if em_c3 == 'free':\n",
    "        embedding3 = Embedding(vocab_size, free_em_dim)(inputs3)\n",
    "    else:\n",
    "        embedding3 = Embedding(vocab_size, len(eval(em_c3)[0]), weights = [eval(em_c3)],input_length=length,trainable = em_trainable_flag_c3)(inputs3)\n",
    "    conv3 = Conv1D(filters=n_filters, kernel_size=filter_size_c3, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(dropout)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(n_dense, activation='relu')(merged)\n",
    "    outputs = Dense(2, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # summarize\n",
    "#     print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 700\n",
      "Vocabulary size: 46558\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(trainX)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % max_len)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "trainX = encode_text(tokenizer, trainX, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google news word2vec\n"
     ]
    }
   ],
   "source": [
    "# glove_model = load_GloVe_embedding('word_embeddings/glove.6B.300d.txt')\n",
    "# fast_text_model = load_fast_text_model(train_sentences)\n",
    "# godin_model = load_godin_word_embedding(\"word_embeddings/word2vec_twitter_model.bin\")\n",
    "word2vec_model= load_google_word2vec('word_embeddings/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding_matrix_glove = get_GloVe_embedding_matrix(glove_model)\n",
    "embedding_matrix_word2vec = get_word2vec_embedding_matrix(word2vec_model)\n",
    "# embedding_matrix_fast_text = get_fast_text_matrix(fast_text_model)\n",
    "# embedding_matrix_godin = get_godin_embedding_matrix(godin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',name='learning_rate')\n",
    "\n",
    "para_dropout = Real(low=0.4, high=0.9,name = 'dropout')\n",
    "\n",
    "para_n_dense = Categorical(categories=[100,200,300,400], name='n_dense')\n",
    "\n",
    "para_n_filters = Categorical(categories=[32,64,100,200,300,400],name='n_filters')\n",
    "\n",
    "para_filter_size_c1 = Integer(low=1,high=6,name = 'filter_size_c1')\n",
    "para_filter_size_c2 = Integer(low=1,high=6,name = 'filter_size_c2')\n",
    "para_filter_size_c3 = Integer(low=1,high=6,name = 'filter_size_c3')\n",
    "\n",
    "para_em_c1 = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em_c1')\n",
    "para_em_c2 = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em_c2')\n",
    "para_em_c3 = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em_c3')\n",
    "\n",
    "para_em_trainable_flag_c1 = Categorical(categories=[True,False],name='em_trainable_flag_c1')\n",
    "para_em_trainable_flag_c2 = Categorical(categories=[True,False],name='em_trainable_flag_c2')\n",
    "para_em_trainable_flag_c3 = Categorical(categories=[True,False],name='em_trainable_flag_c3')\n",
    "\n",
    "para_free_em_dim = Categorical(categories=[100,300,400],name='free_em_dim')\n",
    "\n",
    "para_batch_size = Categorical(categories=[8,16,32,64],name='batch_size')\n",
    "\n",
    "# para_epoch = Categorical(categories=[10,20,30,50,100],name='epoch')\n",
    "para_epoch = Categorical(categories=[1,2],name='epoch')\n",
    "\n",
    "\n",
    "parameters = [para_learning_rate,para_dropout,para_n_dense,para_n_filters,para_filter_size_c1,para_filter_size_c2,para_filter_size_c3,para_em_c1,para_em_c2,para_em_c3,para_em_trainable_flag_c1,para_em_trainable_flag_c2,para_em_trainable_flag_c3,para_free_em_dim,para_batch_size,para_epoch]\n",
    "\n",
    "default_parameters = [0.001,0.5,100,32,2,4,6,'embedding_matrix_word2vec','embedding_matrix_word2vec','embedding_matrix_word2vec',False,False,True,100,16,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = 1\n",
    "record = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=parameters)\n",
    "def fitness(learning_rate,dropout,n_dense,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,em_trainable_flag_c1,em_trainable_flag_c2,em_trainable_flag_c3,free_em_dim,batch_size,epoch):\n",
    "    global key\n",
    "    global record\n",
    "    print('-----------------------------combination no={0}------------------'.format(key))\n",
    "    \n",
    "    parameters = {\n",
    "            \"n_dense\": n_dense,\n",
    "            \"dropout\": dropout,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"n_filters\": n_filters,\n",
    "            \"filter_size_c1\": filter_size_c1,\n",
    "            \"filter_size_c2\": filter_size_c2,\n",
    "            \"filter_size_c3\": filter_size_c3,\n",
    "            \"em_c1\": em_c1,\n",
    "            \"em_c2\": em_c2,\n",
    "            \"em_c3\": em_c3,\n",
    "            \"free_em_dim\": free_em_dim,\n",
    "            \"em_trainable_flag_c1\": em_trainable_flag_c1,\n",
    "            \"em_trainable_flag_c2\": em_trainable_flag_c2,\n",
    "            \"em_trainable_flag_c3\": em_trainable_flag_c3,\n",
    "            \"batch\": batch_size,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "    \n",
    "    \n",
    "    pprint(parameters)\n",
    "    \n",
    "    itr = 1\n",
    "    acc_record = []\n",
    "    itr_record = {}\n",
    "    for train,test in kfold.split(trainX,trainY):\n",
    "        print(\"k fold validation itr == {0}\".format(itr))\n",
    "        X = trainX[train]\n",
    "        Y = to_categorical(trainY[train],num_classes=2)\n",
    "        X_ = trainX[test]\n",
    "        Y_ = to_categorical(trainY[test],num_classes=2)\n",
    "        model = define_model(length = max_len,\n",
    "                             vocab_size=vocab_size,\n",
    "                             n_dense = parameters[\"n_dense\"],\n",
    "                             dropout = parameters[\"dropout\"],\n",
    "                             learning_rate = parameters[\"learning_rate\"],\n",
    "                             n_filters = parameters[\"n_filters\"],\n",
    "                             filter_size_c1 = parameters[\"filter_size_c1\"],\n",
    "                             filter_size_c2 = parameters[\"filter_size_c2\"],\n",
    "                             filter_size_c3 = parameters[\"filter_size_c3\"],\n",
    "                             em_c1 = parameters[\"em_c1\"],\n",
    "                             em_c2 = parameters[\"em_c1\"],\n",
    "                             em_c3 = parameters[\"em_c1\"],\n",
    "                             free_em_dim = parameters[\"free_em_dim\"],\n",
    "                             em_trainable_flag_c1 = parameters[\"em_trainable_flag_c1\"],\n",
    "                             em_trainable_flag_c2 = parameters[\"em_trainable_flag_c2\"],\n",
    "                             em_trainable_flag_c3 = parameters[\"em_trainable_flag_c3\"])\n",
    "        history = model.fit([X,X,X],Y,validation_data = [[X_,X_,X_],Y_],epochs=parameters[\"epoch\"],batch_size=parameters[\"batch\"])\n",
    "        pred = model.predict([X_,X_,X_])\n",
    "        print(history.history)\n",
    "        for x in pred:\n",
    "            print(x.argmax())\n",
    "        acc = history.history['val_acc'][-1]\n",
    "        print(acc)\n",
    "        acc_record.append(acc)\n",
    "        itr_record[itr] = {}\n",
    "        itr_record[itr][\"acc\"] = acc\n",
    "        model.save('models/'+str(key)+'_'+str(itr)+'.h5')\n",
    "        itr+=1\n",
    "    record[key] = {}\n",
    "    record[key][\"parameter\"] = parameters\n",
    "    mean_acc = np.mean(acc_record)\n",
    "    record[key][\"mean_acc\"] = mean_acc\n",
    "    record[key][\"itr_record\"] = itr_record\n",
    "\n",
    "    with open(\"models/record.json\",'w')as fout:\n",
    "        json.dump(record,fout,indent=4)\n",
    "    key+=1\n",
    "    \n",
    "    del model\n",
    "    K.clear_session()\n",
    "    \n",
    "    return -mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------combination no=1------------------\n",
      "{'batch': 16,\n",
      " 'dropout': 0.5,\n",
      " 'em_c1': 'embedding_matrix_word2vec',\n",
      " 'em_c2': 'embedding_matrix_word2vec',\n",
      " 'em_c3': 'embedding_matrix_word2vec',\n",
      " 'em_trainable_flag_c1': False,\n",
      " 'em_trainable_flag_c2': False,\n",
      " 'em_trainable_flag_c3': True,\n",
      " 'epoch': 2,\n",
      " 'filter_size_c1': 2,\n",
      " 'filter_size_c2': 4,\n",
      " 'filter_size_c3': 6,\n",
      " 'free_em_dim': 100,\n",
      " 'learning_rate': 0.001,\n",
      " 'n_dense': 100,\n",
      " 'n_filters': 32}\n",
      "k fold validation itr == 1\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.7436 - acc: 0.5245 - val_loss: 0.6913 - val_acc: 0.5250\n",
      "Epoch 2/2\n",
      " 688/1000 [===================>..........] - ETA: 9s - loss: 0.6789 - acc: 0.5618"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters,\n",
    "                            acq_func='EI',\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_parameters():\n",
    "#     #range values\n",
    "#     para_n_dense = [100,200,300,400]\n",
    "#     para_n_filters = [100,200,300,400]\n",
    "#     para_filter_size = [1,2,3,4,5,6]\n",
    "# #     para_em = ['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free']\n",
    "#     para_em = ['embedding_matrix_word2vec']\n",
    "#     para_free_em_dim = [100,300,400]\n",
    "#     para_em_trainable_flag = [True,False]\n",
    "#     para_batch_size = [8,16,32,64]\n",
    "# #     para_epoc = [10,30,60,100]\n",
    "#     para_epoc = [10]\n",
    "# #     para_batch_size = [64]\n",
    "#     #selecting_random_value\n",
    "#     parameters = {\"n_dense\": choice(para_n_dense),\n",
    "#             \"dropout\": uniform(0.4, 0.9),\n",
    "#             \"learning_rate\": uniform(0.0001, 0.01),\n",
    "#             \"n_filters\": choice(para_n_filters),\n",
    "#             \"filter_size_c1\": choice(para_filter_size),\n",
    "#             \"filter_size_c2\": choice(para_filter_size),\n",
    "#             \"filter_size_c3\": choice(para_filter_size),\n",
    "#             \"em_c1\": choice(para_em),\n",
    "#             \"em_c2\": choice(para_em),\n",
    "#             \"em_c3\": choice(para_em),\n",
    "#             \"free_em_dim\": choice(para_free_em_dim),\n",
    "#             \"em_trainable_flag_c1\": choice(para_em_trainable_flag),\n",
    "#             \"em_trainable_flag_c2\": choice(para_em_trainable_flag),\n",
    "#             \"em_trainable_flag_c3\": choice(para_em_trainable_flag),\n",
    "#             \"batch\": choice(para_batch_size),\n",
    "#             \"epoch\": choice(para_epoc)\n",
    "#         }\n",
    "#     return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for _ in range(number_of_models):\n",
    "#     itr = 1\n",
    "#     f1_record = []\n",
    "#     p_record = []\n",
    "#     r_record = []\n",
    "#     itr_record = {}\n",
    "#     parameters = get_parameters()\n",
    "#     parameters = {\n",
    "#             \"n_dense\": 400,\n",
    "#             \"dropout\": 0.5777195655120914,\n",
    "#             \"learning_rate\": 0.0071353667446707675,\n",
    "#             \"n_filters\": 100,\n",
    "#             \"filter_size_c1\": 6,\n",
    "#             \"filter_size_c2\": 4,\n",
    "#             \"filter_size_c3\": 4,\n",
    "#             \"em_c1\": \"embedding_matrix_word2vec\",\n",
    "#             \"em_c2\": \"embedding_matrix_word2vec\",\n",
    "#             \"em_c3\": \"embedding_matrix_word2vec\",\n",
    "#             \"free_em_dim\": 400,\n",
    "#             \"em_trainable_flag_c1\": False,\n",
    "#             \"em_trainable_flag_c2\": True,\n",
    "#             \"em_trainable_flag_c3\": False,\n",
    "#             \"batch\": 16,\n",
    "#             \"epoch\": 1\n",
    "#         }\n",
    "#     print(\"model number {0}\".format(key))\n",
    "#     print(parameters)\n",
    "#     for train,test in kfold.split(trainX,trainY):\n",
    "#         print(\"k fold validation itr == {0}\".format(itr))\n",
    "#         X = trainX[train]\n",
    "#         Y = to_categorical(trainY[train],num_classes=3)\n",
    "#         X_ = trainX[test]\n",
    "#         Y_ = list(trainY[test])\n",
    "#         model = define_model(length = max_len,\n",
    "#                              vocab_size=vocab_size,\n",
    "#                              n_dense = parameters[\"n_dense\"],\n",
    "#                              dropout = parameters[\"dropout\"],\n",
    "#                              learning_rate = parameters[\"learning_rate\"],\n",
    "#                              n_filters = parameters[\"n_filters\"],\n",
    "#                              filter_size_c1 = parameters[\"filter_size_c1\"],\n",
    "#                              filter_size_c2 = parameters[\"filter_size_c2\"],\n",
    "#                              filter_size_c3 = parameters[\"filter_size_c3\"],\n",
    "#                              em_c1 = parameters[\"em_c1\"],\n",
    "#                              em_c2 = parameters[\"em_c1\"],\n",
    "#                              em_c3 = parameters[\"em_c1\"],\n",
    "#                              free_em_dim = parameters[\"free_em_dim\"],\n",
    "#                              em_trainable_flag_c1 = parameters[\"em_trainable_flag_c1\"],\n",
    "#                              em_trainable_flag_c2 = parameters[\"em_trainable_flag_c2\"],\n",
    "#                              em_trainable_flag_c3 = parameters[\"em_trainable_flag_c3\"])\n",
    "#         history = model.fit([X,X,X],Y,epochs=parameters[\"epoch\"],batch_size=parameters[\"batch\"])\n",
    "#         pred = model.predict([X_,X_,X_])\n",
    "#         pred_labels = [x.argmax() for x in pred]\n",
    "#         for foo in zip(Y_[:50],pred_labels[:50]):\n",
    "#             print(foo)\n",
    "#         f1 = f1_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "#         p = precision_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "#         r = recall_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "#         print(f1,p,r)\n",
    "#         f1_record.append(f1)\n",
    "#         p_record.append(p)\n",
    "#         r_record.append(r)\n",
    "#         itr_record[itr] = {}\n",
    "#         itr_record[itr][\"f1\"] = f1\n",
    "#         itr_record[itr][\"p\"] = p\n",
    "#         itr_record[itr][\"r\"] = r\n",
    "#         model.save('models/'+str(key)+'_'+str(itr)+'.h5')\n",
    "#         itr+=1\n",
    "#     record[key] = {}\n",
    "#     record[key][\"parameter\"] = parameters\n",
    "#     record[key][\"mean_f1\"] = np.mean(f1_record)\n",
    "#     record[key][\"itr_record\"] = itr_record\n",
    "\n",
    "#     with open(\"models/record.json\",'w')as fout:\n",
    "#         json.dump(record,fout,indent=4)\n",
    "#     key+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
