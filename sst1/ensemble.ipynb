{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import json\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import remove\n",
    "from pprint import pprint\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from gensim.models import KeyedVectors\n",
    "import word2vecReader as godin_embedding\n",
    "import fasttext\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from base_learners import cnn,lstm,bi_lstm,cnn_bi_lstm,cnn_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_file(filename):\n",
    "    with open(filename,'r', errors='ignore') as fin:\n",
    "        lines = fin.readlines()\n",
    "    label = [int(x.split()[0]) for x in lines]\n",
    "    sentence = [' '.join(x.split()[1:]) for x in lines]\n",
    "    return label,sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels,train_sentences = load_data_from_file('dataset/sst1/stsa.fine.train')\n",
    "dev_label,dev_sentence = load_data_from_file('dataset/sst1/stsa.fine.dev')\n",
    "test_labels,test_sentences = load_data_from_file('dataset/sst1/stsa.fine.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = train_sentences+dev_sentence\n",
    "train_labels = train_labels+dev_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9645, 9645, 2210, 2210)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels),len(train_sentences),len(test_labels),len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train_labels[:500]\n",
    "train_sentences = train_sentences[:500]\n",
    "test_labels=test_labels[:100]\n",
    "test_sentences = test_sentences[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_classes = len(set(train_labels))\n",
    "number_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500, 100, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels),len(train_sentences),len(test_labels),len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    list_punctuation = list(string.punctuation)\n",
    "    for i in list_punctuation:\n",
    "        s = s.replace(i,'')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    # should have used translate but for some reason it breaks on my server\n",
    "    tokens = [remove_punctuation(w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning data\n"
     ]
    }
   ],
   "source": [
    "print(\"cleaning data\")\n",
    "trainX = [clean_sentence(s) for s in train_sentences]\n",
    "testX = [clean_sentence(s) for s in test_sentences]\n",
    "trainY = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_godin_word_embedding(path):\n",
    "    print(\"Loading Goding model.\")\n",
    "    return godin_embedding.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_google_word2vec(file_name):\n",
    "    print(\"Loading google news word2vec\")\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_embedding_matrix(model,dim):\n",
    "    #dim = 300 for google word2vec\n",
    "    #dim = 400 for godin\n",
    "    #dim = 100 for fast text\n",
    "    embedding_matrix = np.zeros((vocab_size,dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 24\n",
      "Vocabulary size: 17103\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(trainX)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % max_len)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "trainX = encode_text(tokenizer, trainX, max_len)\n",
    "testX = encode_text(tokenizer, testX, max_len)\n",
    "trainY = to_categorical(trainY,num_classes=number_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google news word2vec\n"
     ]
    }
   ],
   "source": [
    "# godin_model = load_godin_word_embedding(\"../word_embeddings/word2vec_twitter_model.bin\")\n",
    "word2vec_model= load_google_word2vec('../word_embeddings/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix_word2vec = get_word_embedding_matrix(word2vec_model,300)\n",
    "# embedding_matrix_godin = get_word_embedding_matrix(godin_model,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn\n",
    "# 0.4710\n",
    "def init_cnn():\n",
    "    cnn_parameter = {'batch': 8,\n",
    "                'dropout': 0.6,\n",
    "                'em': 'embedding_matrix_word2vec',\n",
    "                'em_trainable_flag': True,\n",
    "                'epoch': 10,\n",
    "                'filter_size': 6,\n",
    "                'learning_rate': 0.0001,\n",
    "                'n_dense': 200,\n",
    "                'n_filters': 100}\n",
    "    return cnn(length=max_len,\n",
    "               vocab_size=vocab_size,\n",
    "               learning_rate=cnn_parameter['learning_rate'],\n",
    "               n_dense=cnn_parameter['n_dense'],\n",
    "               dropout=cnn_parameter['dropout'],\n",
    "               n_filters=cnn_parameter['n_filters'],\n",
    "               filter_size=cnn_parameter['filter_size'],\n",
    "               em=eval(cnn_parameter['em']),\n",
    "               number_of_classes=number_of_classes,\n",
    "               em_trainable_flag=cnn_parameter['em_trainable_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  lstm\n",
    "# 0.4701\n",
    "def init_lstm():\n",
    "    lstm_parameter={'batch': 64,\n",
    "                    'dropout': 0.6,\n",
    "                    'em': 'embedding_matrix_word2vec',\n",
    "                    'em_trainable_flag': False,\n",
    "                    'epoch': 20,\n",
    "                    'learning_rate': 0.0034157107277860235,\n",
    "                    'units_out': 128}\n",
    "    return lstm(length=max_len,\n",
    "                vocab_size=vocab_size,\n",
    "                learning_rate=lstm_parameter['learning_rate'],\n",
    "                dropout=lstm_parameter['dropout'],\n",
    "                units_out=lstm_parameter['units_out'],\n",
    "                em=eval(lstm_parameter['em']),\n",
    "                number_of_classes=number_of_classes,\n",
    "                em_trainable_flag=lstm_parameter['em_trainable_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi_lstm\n",
    "# 0.4529\n",
    "def init_bi_lstm():\n",
    "    bi_lstm_parameter={'batch':8,\n",
    "                 'dropout': 0.6,\n",
    "                 'em': 'embedding_matrix_word2vec',\n",
    "                 'em_trainable_flag': False,\n",
    "                 'epoch': 5,\n",
    "                 'learning_rate': 0.0001,\n",
    "                 'units_out': 256}\n",
    "    return bi_lstm(length=max_len,\n",
    "                vocab_size=vocab_size,\n",
    "                learning_rate=bi_lstm_parameter['learning_rate'],\n",
    "                dropout=bi_lstm_parameter['dropout'],\n",
    "                units_out=bi_lstm_parameter['units_out'],\n",
    "                em=eval(bi_lstm_parameter['em']),\n",
    "                number_of_classes=number_of_classes,\n",
    "                em_trainable_flag=bi_lstm_parameter['em_trainable_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_lstm\n",
    "# 0.4179\n",
    "def init_cnn_lstm():\n",
    "    cnn_lstm_parameter={'batch': 8,\n",
    "                    'conv_dropout': 0.5,\n",
    "                    'em': 'embedding_matrix_word2vec',\n",
    "                    'em_trainable_flag': False,\n",
    "                    'epoch': 10,\n",
    "                    'filter_size': 1,\n",
    "                    'learning_rate': 0.001,\n",
    "                    'lstm_dropout': 0.4,\n",
    "                    'n_filters': 100,\n",
    "                    'units_out': 64}\n",
    "    return cnn_lstm(length=max_len,\n",
    "                    vocab_size=vocab_size,\n",
    "                    learning_rate=cnn_lstm_parameter['learning_rate'],\n",
    "                    n_filters=cnn_lstm_parameter['n_filters'],\n",
    "                    filter_size=cnn_lstm_parameter['filter_size'],\n",
    "                    em=eval(cnn_lstm_parameter['em']),\n",
    "                    number_of_classes=number_of_classes,\n",
    "                    em_trainable_flag=cnn_lstm_parameter['em_trainable_flag'],\n",
    "                    conv_dropout=cnn_lstm_parameter['conv_dropout'],\n",
    "                    l_or_g_dropout=cnn_lstm_parameter['lstm_dropout'],\n",
    "                    units_out=cnn_lstm_parameter['units_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn_bi_lstm\n",
    "# 0.4705\n",
    "def init_cnn_bi_lstm():\n",
    "    cnn_bi_lstm_parameter={'batch': 8,\n",
    "                    'conv_dropout': 0.5,\n",
    "                    'em': 'embedding_matrix_word2vec',\n",
    "                    'em_trainable_flag': False,\n",
    "                    'epoch': 5,\n",
    "                    'filter_size': 1,\n",
    "                    'learning_rate': 0.001,\n",
    "                    'lstm_dropout': 0.2,\n",
    "                    'n_filters': 100,\n",
    "                    'units_out': 64}\n",
    "    m = cnn_bi_lstm(length=max_len,\n",
    "                    vocab_size=vocab_size,\n",
    "                    learning_rate=cnn_bi_lstm_parameter['learning_rate'],\n",
    "                    n_filters=cnn_bi_lstm_parameter['n_filters'],\n",
    "                    filter_size=cnn_bi_lstm_parameter['filter_size'],\n",
    "                    em=eval(cnn_bi_lstm_parameter['em']),\n",
    "                    number_of_classes=number_of_classes,\n",
    "                    em_trainable_flag=cnn_bi_lstm_parameter['em_trainable_flag'],\n",
    "                    conv_dropout=cnn_bi_lstm_parameter['conv_dropout'],\n",
    "                    l_or_g_dropout=cnn_bi_lstm_parameter['lstm_dropout'],\n",
    "                    units_out=cnn_bi_lstm_parameter['units_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pred_of_model(m):\n",
    "    history = m.fit(trainX,trainY,epochs=100,verbose=0)\n",
    "    y_pred = m.predict(X_test)\n",
    "    y_test_class = np.argmax(y_test,axis=1)\n",
    "    y_pred_class = np.argmax(y_pred,axis=1)\n",
    "    print(accuracy_score(y_test_class,y_pred_class))\n",
    "    return y_pred_class\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
