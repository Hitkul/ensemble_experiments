{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "from gensim.models import KeyedVectors\n",
    "import word2vecReader as godin_embedding\n",
    "import fasttext\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,Embedding\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from random import uniform,choice\n",
    "from os import remove\n",
    "import re\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to read MR dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def load_data(filename):\n",
    "#     with open(filename,'r') as fin:\n",
    "#         return json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset_dict = load_data(\"dataset/final_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def extract_data(dataset_dict):\n",
    "#     review = [dataset_dict[key][\"review\"] for key in dataset_dict.keys()]\n",
    "#     polarity = [dataset_dict[key][\"polarity\"] for key in dataset_dict.keys()]\n",
    "#     return review, polarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# review, polarity = extract_data(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# review = review[:100]\n",
    "# polarity = polarity[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to read smm4h dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_file(filename):\n",
    "    data = pd.read_csv(filename, sep=\"\\t\", header=None)\n",
    "    data.columns = [\"tweet_id\", \"username\", \"database_id\", \"class\",\"tweet\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data_from_file('dataset/smm4h/personal_intake_tweets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = load_data_from_file('dataset/smm4h/personal_intake_tweets_dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = train_data['tweet'].tolist()+dev_data['tweet'].tolist()\n",
    "train_labels = train_data['class'].tolist()+dev_data['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9107, 9107)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences),len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences=train_sentences[:200]\n",
    "train_labels = train_labels[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #for PYTHON 2.7\n",
    "    #tokens = [w.translate(None, string.punctuation) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train data\n"
     ]
    }
   ],
   "source": [
    "print(\"cleaning train data\")\n",
    "trainX = [clean_sentence(s) for s in train_sentences]\n",
    "trainY = np.array([l-1 for l in train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = [len(s) for s in trainX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAIMCAYAAAAQMqEjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nADDW/wfQYzBSilkVvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f373bb1ebe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length,bins=[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150])\n",
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading GloVe embedding\n",
    "def load_GloVe_embedding(file_name):\n",
    "    print('Loading GloVe word vectors.')\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "def get_GloVe_embedding_matrix(embeddings_index):\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fast text word embedding\n",
    "def load_fast_text_model(sentences):\n",
    "    try:\n",
    "        m = fasttext.load_model('word_embeddings/fast_text_model.bin')\n",
    "        print(\"trained model loaded\")\n",
    "        return m\n",
    "    except:\n",
    "        print(\"traning new model\")\n",
    "        with open('temp_file.txt','w') as temp_file:\n",
    "            for sentence in sentences:\n",
    "                temp_file.write(sentence)\n",
    "        m = fasttext.cbow('temp_file.txt','word_embeddings/fast_text_model')\n",
    "        remove('temp_file.txt')\n",
    "        print('model trained')\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fast_text_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading godin word embedding\n",
    "def load_godin_word_embedding(path):\n",
    "    print(\"Loading Goding model.\")\n",
    "    return godin_embedding.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_godin_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,400))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading Google Word2Vec\n",
    "def load_google_word2vec(file_name):\n",
    "    print(\"Loading google news word2vec\")\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(length,vocab_size,n_dense,dropout,learning_rate,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,free_em_dim,em_trainable_flag_c1,em_trainable_flag_c2,em_trainable_flag_c3):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    if em_c1 == 'free':\n",
    "        embedding1 = Embedding(vocab_size, free_em_dim)(inputs1)\n",
    "    else:\n",
    "        embedding1 = Embedding(vocab_size, len(eval(em_c1)[0]), weights = [eval(em_c1)],input_length=length,trainable = em_trainable_flag_c1)(inputs1)\n",
    "    \n",
    "    conv1 = Conv1D(filters=n_filters, kernel_size=filter_size_c1, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(dropout)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    if em_c2 == 'free':\n",
    "        embedding2 = Embedding(vocab_size, free_em_dim)(inputs2)\n",
    "    else:\n",
    "        embedding2 = Embedding(vocab_size, len(eval(em_c2)[0]), weights = [eval(em_c2)],input_length=length,trainable = em_trainable_flag_c2)(inputs2)\n",
    "    conv2 = Conv1D(filters=n_filters, kernel_size=filter_size_c2, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(dropout)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    if em_c3 == 'free':\n",
    "        embedding3 = Embedding(vocab_size, free_em_dim)(inputs3)\n",
    "    else:\n",
    "        embedding3 = Embedding(vocab_size, len(eval(em_c3)[0]), weights = [eval(em_c3)],input_length=length,trainable = em_trainable_flag_c3)(inputs3)\n",
    "    conv3 = Conv1D(filters=n_filters, kernel_size=filter_size_c3, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(dropout)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(n_dense, activation='relu')(merged)\n",
    "    outputs = Dense(3, activation='softmax')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # summarize\n",
    "#     print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 150\n",
      "Vocabulary size: 879\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(trainX)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % max_len)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "trainX = encode_text(tokenizer, trainX, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained model loaded\n"
     ]
    }
   ],
   "source": [
    "# glove_model = load_GloVe_embedding('word_embeddings/glove.6B.300d.txt')\n",
    "fast_text_model = load_fast_text_model(train_sentences)\n",
    "# godin_model = load_godin_word_embedding(\"word_embeddings/word2vec_twitter_model.bin\")\n",
    "# word2vec_model= load_google_word2vec('word_embeddings/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding_matrix_glove = get_GloVe_embedding_matrix(glove_model)\n",
    "# embedding_matrix_word2vec = get_word2vec_embedding_matrix(word2vec_model)\n",
    "embedding_matrix_fast_text = get_fast_text_matrix(fast_text_model)\n",
    "# embedding_matrix_godin = get_godin_embedding_matrix(godin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parameters():\n",
    "    #range values\n",
    "    para_n_dense = [100,200,300,400]\n",
    "    para_n_filters = [100,200,300,400]\n",
    "    para_filter_size = [1,2,3,4,5,6]\n",
    "#     para_em = ['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free']\n",
    "    para_em = ['embedding_matrix_fast_text','free']\n",
    "    para_free_em_dim = [100,300,400]\n",
    "    para_em_trainable_flag = [True,False]\n",
    "#     para_batch_size = [8,16,32,64]\n",
    "#     para_epoc = [10,30,60,100]\n",
    "    para_epoc = [1]\n",
    "    para_batch_size = [64]\n",
    "    #selecting_random_value\n",
    "    parameters = {\"n_dense\": choice(para_n_dense),\n",
    "            \"dropout\": uniform(0.4, 0.9),\n",
    "            \"learning_rate\": uniform(0.0001, 0.1),\n",
    "            \"n_filters\": choice(para_n_filters),\n",
    "            \"filter_size_c1\": choice(para_filter_size),\n",
    "            \"filter_size_c2\": choice(para_filter_size),\n",
    "            \"filter_size_c3\": choice(para_filter_size),\n",
    "            \"em_c1\": choice(para_em),\n",
    "            \"em_c2\": choice(para_em),\n",
    "            \"em_c3\": choice(para_em),\n",
    "            \"free_em_dim\": choice(para_free_em_dim),\n",
    "            \"em_trainable_flag_c1\": choice(para_em_trainable_flag),\n",
    "            \"em_trainable_flag_c2\": choice(para_em_trainable_flag),\n",
    "            \"em_trainable_flag_c3\": choice(para_em_trainable_flag),\n",
    "            \"batch\": choice(para_batch_size),\n",
    "            \"epoch\": choice(para_epoc)\n",
    "        }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = 1\n",
    "record = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_models = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model number 1\n",
      "{'n_dense': 400, 'dropout': 0.5939744427390881, 'learning_rate': 0.039335310634092666, 'n_filters': 100, 'filter_size_c1': 6, 'filter_size_c2': 1, 'filter_size_c3': 2, 'em_c1': 'embedding_matrix_fast_text', 'em_c2': 'free', 'em_c3': 'free', 'free_em_dim': 400, 'em_trainable_flag_c1': False, 'em_trainable_flag_c2': True, 'em_trainable_flag_c3': True, 'batch': 64, 'epoch': 1}\n",
      "k fold validation itr == 1\n",
      "Epoch 1/1\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 2.4625 - acc: 0.6566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.0\n",
      "k fold validation itr == 2\n",
      "Epoch 1/1\n",
      "101/101 [==============================] - 2s 15ms/step - loss: 2.4137 - acc: 0.6634\n",
      "0.0 0.0 0.0\n",
      "model number 2\n",
      "{'n_dense': 400, 'dropout': 0.7726816394341673, 'learning_rate': 0.02964585422586822, 'n_filters': 400, 'filter_size_c1': 1, 'filter_size_c2': 5, 'filter_size_c3': 5, 'em_c1': 'free', 'em_c2': 'free', 'em_c3': 'embedding_matrix_fast_text', 'free_em_dim': 300, 'em_trainable_flag_c1': False, 'em_trainable_flag_c2': False, 'em_trainable_flag_c3': False, 'batch': 64, 'epoch': 1}\n",
      "k fold validation itr == 1\n",
      "Epoch 1/1\n",
      "99/99 [==============================] - 5s 55ms/step - loss: 2.3509 - acc: 0.6633\n",
      "0.0 0.0 0.0\n",
      "k fold validation itr == 2\n",
      "Epoch 1/1\n",
      "101/101 [==============================] - 6s 56ms/step - loss: 3.0493 - acc: 0.6238\n",
      "0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "for _ in range(number_of_models):\n",
    "    itr = 1\n",
    "    f1_record = []\n",
    "    p_record = []\n",
    "    r_record = []\n",
    "    itr_record = {}\n",
    "    parameters = get_parameters()\n",
    "    print(\"model number {0}\".format(key))\n",
    "    print(parameters)\n",
    "    for train,test in kfold.split(trainX,trainY):\n",
    "        print(\"k fold validation itr == {0}\".format(itr))\n",
    "        X = trainX[train]\n",
    "        Y = to_categorical(trainY[train],num_classes=3)\n",
    "        X_ = trainX[test]\n",
    "        Y_ = list(trainY[test])\n",
    "        model = define_model(length = max_len,\n",
    "                             vocab_size=vocab_size,\n",
    "                             n_dense = parameters[\"n_dense\"],\n",
    "                             dropout = parameters[\"dropout\"],\n",
    "                             learning_rate = parameters[\"learning_rate\"],\n",
    "                             n_filters = parameters[\"n_filters\"],\n",
    "                             filter_size_c1 = parameters[\"filter_size_c1\"],\n",
    "                             filter_size_c2 = parameters[\"filter_size_c2\"],\n",
    "                             filter_size_c3 = parameters[\"filter_size_c3\"],\n",
    "                             em_c1 = parameters[\"em_c1\"],\n",
    "                             em_c2 = parameters[\"em_c1\"],\n",
    "                             em_c3 = parameters[\"em_c1\"],\n",
    "                             free_em_dim = parameters[\"free_em_dim\"],\n",
    "                             em_trainable_flag_c1 = parameters[\"em_trainable_flag_c1\"],\n",
    "                             em_trainable_flag_c2 = parameters[\"em_trainable_flag_c2\"],\n",
    "                             em_trainable_flag_c3 = parameters[\"em_trainable_flag_c3\"])\n",
    "        history = model.fit([X,X,X],Y,epochs=parameters[\"epoch\"],batch_size=parameters[\"batch\"])\n",
    "        pred = model.predict([X_,X_,X_])\n",
    "        pred_labels = [x.argmax() for x in pred]\n",
    "        f1 = f1_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "        p = precision_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "        r = recall_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "        print(f1,p,r)\n",
    "        f1_record.append(f1)\n",
    "        p_record.append(p)\n",
    "        r_record.append(r)\n",
    "        itr_record[itr] = {}\n",
    "        itr_record[itr][\"f1\"] = f1\n",
    "        itr_record[itr][\"p\"] = p\n",
    "        itr_record[itr][\"r\"] = r\n",
    "        model.save('models/'+str(key)+'_'+str(itr)+'.h5')\n",
    "        itr+=1\n",
    "    record[key] = {}\n",
    "    record[key][\"parameter\"] = parameters\n",
    "    record[key][\"mean_f1\"] = np.mean(f1_record)\n",
    "    record[key][\"itr_record\"] = itr_record\n",
    "\n",
    "    with open(\"models/record.json\",'w')as fout:\n",
    "        json.dump(record,fout,indent=4)\n",
    "    key+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
