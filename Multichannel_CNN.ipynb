{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "from gensim.models import KeyedVectors\n",
    "import word2vecReader as godin_embedding\n",
    "import fasttext\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,Embedding\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from random import uniform,choice\n",
    "from os import remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename,'r') as fin:\n",
    "        return json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_dict = load_data(\"dataset/final_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(dataset_dict):\n",
    "    review = [dataset_dict[key][\"review\"] for key in dataset_dict.keys()]\n",
    "    polarity = [dataset_dict[key][\"polarity\"] for key in dataset_dict.keys()]\n",
    "    return review, polarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review, polarity = extract_data(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# review = review[:100]\n",
    "# polarity = polarity[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review),len(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #for PYTHON 2.7\n",
    "    #tokens = [w.translate(None, string.punctuation) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review = [clean_sentence(s) for s in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = [len(s) for s in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2., 11., 22., 32., 15., 10.,  3.,  1.,  2.,  2.]),\n",
       " array([ 203. ,  879.5, 1556. , 2232.5, 2909. , 3585.5, 4262. , 4938.5,\n",
       "        5615. , 6291.5, 6968. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAIMCAYAAAAQMqEjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wNdKVjqYBjxQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7f90646550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 6500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading GloVe embedding\n",
    "def load_GloVe_embedding(file_name):\n",
    "    print('Loading GloVe word vectors.')\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "def get_GloVe_embedding_matrix(embeddings_index):\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fast text word embedding\n",
    "def load_fast_text_model(sentences):\n",
    "    try:\n",
    "        m = fasttext.load_model('word_embeddings/fast_text_model.bin')\n",
    "        print(\"trained model loaded\")\n",
    "        return m\n",
    "    except:\n",
    "        print(\"traning new model\")\n",
    "        with open('temp_file.txt','w') as temp_file:\n",
    "            for sentence in sentences:\n",
    "                temp_file.write(sentence)\n",
    "        m = fasttext.cbow('temp_file.txt','word_embeddings/fast_text_model')\n",
    "        remove('temp_file.txt')\n",
    "        print('model trained')\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fast_text_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading godin word embedding\n",
    "def load_godin_word_embedding(path):\n",
    "    print(\"Loading Goding model.\")\n",
    "    return godin_embedding.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_godin_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,400))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading Google Word2Vec\n",
    "def load_google_word2vec(file_name):\n",
    "    print(\"Loading google news word2vec\")\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(length,vocab_size,n_dense,dropout,learning_rate,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,free_em_dim,em_trainable_flag_c1,em_trainable_flag_c2,em_trainable_flag_c3):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    if em_c1 == 'free':\n",
    "        embedding1 = Embedding(vocab_size, free_em_dim)(inputs1)\n",
    "    else:\n",
    "        embedding1 = Embedding(vocab_size, len(eval(em_c1)[0]), weights = [eval(em_c1)],input_length=length,trainable = em_trainable_flag_c1)(inputs1)\n",
    "    \n",
    "    conv1 = Conv1D(filters=n_filters, kernel_size=filter_size_c1, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(dropout)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    if em_c2 == 'free':\n",
    "        embedding2 = Embedding(vocab_size, free_em_dim)(inputs2)\n",
    "    else:\n",
    "        embedding2 = Embedding(vocab_size, len(eval(em_c2)[0]), weights = [eval(em_c2)],input_length=length,trainable = em_trainable_flag_c2)(inputs2)\n",
    "    conv2 = Conv1D(filters=n_filters, kernel_size=filter_size_c2, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(dropout)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    if em_c3 == 'free':\n",
    "        embedding3 = Embedding(vocab_size, free_em_dim)(inputs3)\n",
    "    else:\n",
    "        embedding3 = Embedding(vocab_size, len(eval(em_c3)[0]), weights = [eval(em_c3)],input_length=length,trainable = em_trainable_flag_c3)(inputs3)\n",
    "    conv3 = Conv1D(filters=n_filters, kernel_size=filter_size_c3, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(dropout)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(n_dense, activation='relu')(merged)\n",
    "    outputs = Dense(2, activation='softmax')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # summarize\n",
    "#     print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = review\n",
    "Y = np.asarray(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 500\n",
      "Vocabulary size: 10441\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(X)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % max_len)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "X = encode_text(tokenizer, X, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_model = load_GloVe_embedding('word_embeddings/glove.6B.300d.txt')\n",
    "# fast_text_model = load_fast_text_model(review)\n",
    "# godin_model = load_godin_word_embedding(\"word_embeddings/word2vec_twitter_model.bin\")\n",
    "# word2vec_model= load_google_word2vec('word_embeddings/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix_glove = get_GloVe_embedding_matrix(glove_model)\n",
    "# embedding_matrix_word2vec = get_word2vec_embedding_matrix(word2vec_model)\n",
    "# embedding_matrix_fast_text = get_fast_text_matrix(fast_text_model)\n",
    "# embedding_matrix_godin = get_godin_embedding_matrix(godin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parameters():\n",
    "    #range values\n",
    "    para_n_dense = [100,200,300,400]\n",
    "    para_n_filters = [100,200,300,400]\n",
    "    para_filter_size = [1,2,3,4,5,6]\n",
    "#     para_em = ['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free']\n",
    "    para_em = ['embedding_matrix_glove','free']\n",
    "    para_free_em_dim = [100,300,400]\n",
    "    para_em_trainable_flag = [True,False]\n",
    "    para_batch_size = [8,16,32,64]\n",
    "    para_epoc = [10,30,60,100]\n",
    "#     para_epoc = [1]\n",
    "#     para_batch_size = [8]\n",
    "    #selecting_random_value\n",
    "    parameters = {\"n_dense\": choice(para_n_dense),\n",
    "            \"dropout\": uniform(0.4, 0.9),\n",
    "            \"learning_rate\": uniform(0.0001, 0.1),\n",
    "            \"n_filters\": choice(para_n_filters),\n",
    "            \"filter_size_c1\": choice(para_n_filters),\n",
    "            \"filter_size_c2\": choice(para_n_filters),\n",
    "            \"filter_size_c3\": choice(para_n_filters),\n",
    "            \"em_c1\": choice(para_em),\n",
    "            \"em_c2\": choice(para_em),\n",
    "            \"em_c3\": choice(para_em),\n",
    "            \"free_em_dim\": choice(para_free_em_dim),\n",
    "            \"em_trainable_flag_c1\": choice(para_em_trainable_flag),\n",
    "            \"em_trainable_flag_c2\": choice(para_em_trainable_flag),\n",
    "            \"em_trainable_flag_c3\": choice(para_em_trainable_flag),\n",
    "            \"batch\": choice(para_batch_size),\n",
    "            \"epoch\": choice(para_epoc)\n",
    "        }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = 1\n",
    "record = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_models = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model number 1\n",
      "k fold validation itr == 1\n",
      "Train on 50 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 107s 2s/step - loss: 7.1618 - acc: 0.5000 - val_loss: 7.6945 - val_acc: 0.5200\n",
      "k fold validation itr == 2\n",
      "Train on 50 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 103s 2s/step - loss: 7.4864 - acc: 0.4200 - val_loss: 8.3357 - val_acc: 0.4800\n",
      "model number 2\n",
      "k fold validation itr == 1\n",
      "Train on 50 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 117s 2s/step - loss: 6.8453 - acc: 0.4800 - val_loss: 7.6945 - val_acc: 0.5200\n",
      "k fold validation itr == 2\n",
      "Train on 50 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 119s 2s/step - loss: 7.4843 - acc: 0.4800 - val_loss: 8.3357 - val_acc: 0.4800\n"
     ]
    }
   ],
   "source": [
    "for _ in range(number_of_models):\n",
    "    itr = 1\n",
    "    acc_record = []\n",
    "    itr_record = {}\n",
    "    parameters = get_parameters()\n",
    "    print(\"model number {0}\".format(key))\n",
    "    for train,test in kfold.split(X,Y):\n",
    "        print(\"k fold validation itr == {0}\".format(itr))\n",
    "        trainX = X[train]\n",
    "        trainY = to_categorical(Y[train],num_classes=2)\n",
    "        testX = X[test]\n",
    "        testY = to_categorical(Y[test],num_classes=2)\n",
    "        model = define_model(length = max_len,\n",
    "                             vocab_size=vocab_size,\n",
    "                             n_dense = parameters[\"n_dense\"],\n",
    "                             dropout = parameters[\"dropout\"],\n",
    "                             learning_rate = parameters[\"learning_rate\"],\n",
    "                             n_filters = parameters[\"n_filters\"],\n",
    "                             filter_size_c1 = parameters[\"filter_size_c1\"],\n",
    "                             filter_size_c2 = parameters[\"filter_size_c2\"],\n",
    "                             filter_size_c3 = parameters[\"filter_size_c3\"],\n",
    "                             em_c1 = parameters[\"em_c1\"],\n",
    "                             em_c2 = parameters[\"em_c1\"],\n",
    "                             em_c3 = parameters[\"em_c1\"],\n",
    "                             free_em_dim = parameters[\"free_em_dim\"],\n",
    "                             em_trainable_flag_c1 = parameters[\"em_trainable_flag_c1\"],\n",
    "                             em_trainable_flag_c2 = parameters[\"em_trainable_flag_c2\"],\n",
    "                             em_trainable_flag_c3 = parameters[\"em_trainable_flag_c3\"])\n",
    "        history = model.fit([trainX,trainX,trainX], trainY, validation_data = [[testX,testX,testX],testY],epochs=parameters[\"epoch\"],batch_size=parameters[\"batch\"])\n",
    "        acc_record.append(history.history[\"val_acc\"][0])\n",
    "        itr_record[itr] = history.history[\"val_acc\"][0]\n",
    "        model.save('models/'+str(key)+'_'+str(itr)+'.h5')\n",
    "        itr+=1\n",
    "    record[key] = {}\n",
    "    record[key][\"parameter\"] = parameters\n",
    "    record[key][\"mean_acc\"] = np.mean(acc_record)\n",
    "    record[key][\"itr_record\"] = itr_record\n",
    "\n",
    "    with open(\"models/record.json\",'w')as fout:\n",
    "        json.dump(record,fout,indent=4)\n",
    "    key+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
