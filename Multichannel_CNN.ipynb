{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "from gensim.models import KeyedVectors\n",
    "import word2vecReader as godin_embedding\n",
    "import fasttext\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,Embedding\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from random import uniform,choice\n",
    "from os import remove\n",
    "import re\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to read MR dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def load_data(filename):\n",
    "#     with open(filename,'r') as fin:\n",
    "#         return json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset_dict = load_data(\"dataset/final_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def extract_data(dataset_dict):\n",
    "#     review = [dataset_dict[key][\"review\"] for key in dataset_dict.keys()]\n",
    "#     polarity = [dataset_dict[key][\"polarity\"] for key in dataset_dict.keys()]\n",
    "#     return review, polarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# review, polarity = extract_data(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# review = review[:100]\n",
    "# polarity = polarity[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to read smm4h dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_file(filename):\n",
    "    data = pd.read_csv(filename, sep=\"\\t\", header=None)\n",
    "    data.columns = [\"tweet_id\", \"username\", \"database_id\", \"class\",\"tweet\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data_from_file('dataset/smm4h/personal_intake_tweets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = load_data_from_file('dataset/smm4h/personal_intake_tweets_dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = train_data['tweet'].tolist()+dev_data['tweet'].tolist()\n",
    "train_labels = train_data['class'].tolist()+dev_data['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9107, 9107)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences),len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1743, 2837, 4527)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.count(1),train_labels.count(2),train_labels.count(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #for PYTHON 2.7\n",
    "    #tokens = [w.translate(None, string.punctuation) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train data\n"
     ]
    }
   ],
   "source": [
    "print(\"cleaning train data\")\n",
    "trainX = [clean_sentence(s) for s in train_sentences]\n",
    "trainY = np.array([l-1 for l in train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = [len(s) for s in trainX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5908"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAAIMCAYAAAAKDkGtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6edad8f588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length,bins=[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150])\n",
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading GloVe embedding\n",
    "def load_GloVe_embedding(file_name):\n",
    "    print('Loading GloVe word vectors.')\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "def get_GloVe_embedding_matrix(embeddings_index):\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fast text word embedding\n",
    "def load_fast_text_model(sentences):\n",
    "    try:\n",
    "        m = fasttext.load_model('word_embeddings/fast_text_model.bin')\n",
    "        print(\"trained model loaded\")\n",
    "        return m\n",
    "    except:\n",
    "        print(\"traning new model\")\n",
    "        with open('temp_file.txt','w') as temp_file:\n",
    "            for sentence in sentences:\n",
    "                temp_file.write(sentence)\n",
    "        m = fasttext.cbow('temp_file.txt','word_embeddings/fast_text_model')\n",
    "        remove('temp_file.txt')\n",
    "        print('model trained')\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fast_text_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading godin word embedding\n",
    "def load_godin_word_embedding(path):\n",
    "    print(\"Loading Goding model.\")\n",
    "    return godin_embedding.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_godin_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,400))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading Google Word2Vec\n",
    "def load_google_word2vec(file_name):\n",
    "    print(\"Loading google news word2vec\")\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(length,vocab_size,n_dense,dropout,learning_rate,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,free_em_dim,em_trainable_flag_c1,em_trainable_flag_c2,em_trainable_flag_c3):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    if em_c1 == 'free':\n",
    "        embedding1 = Embedding(vocab_size, free_em_dim)(inputs1)\n",
    "    else:\n",
    "        embedding1 = Embedding(vocab_size, len(eval(em_c1)[0]), weights = [eval(em_c1)],input_length=length,trainable = em_trainable_flag_c1)(inputs1)\n",
    "    \n",
    "    conv1 = Conv1D(filters=n_filters, kernel_size=filter_size_c1, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(dropout)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    if em_c2 == 'free':\n",
    "        embedding2 = Embedding(vocab_size, free_em_dim)(inputs2)\n",
    "    else:\n",
    "        embedding2 = Embedding(vocab_size, len(eval(em_c2)[0]), weights = [eval(em_c2)],input_length=length,trainable = em_trainable_flag_c2)(inputs2)\n",
    "    conv2 = Conv1D(filters=n_filters, kernel_size=filter_size_c2, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(dropout)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    if em_c3 == 'free':\n",
    "        embedding3 = Embedding(vocab_size, free_em_dim)(inputs3)\n",
    "    else:\n",
    "        embedding3 = Embedding(vocab_size, len(eval(em_c3)[0]), weights = [eval(em_c3)],input_length=length,trainable = em_trainable_flag_c3)(inputs3)\n",
    "    conv3 = Conv1D(filters=n_filters, kernel_size=filter_size_c3, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(dropout)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(n_dense, activation='relu')(merged)\n",
    "    outputs = Dense(3, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # summarize\n",
    "#     print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 150\n",
      "Vocabulary size: 10940\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(trainX)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % max_len)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "trainX = encode_text(tokenizer, trainX, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google news word2vec\n"
     ]
    }
   ],
   "source": [
    "# glove_model = load_GloVe_embedding('word_embeddings/glove.6B.300d.txt')\n",
    "# fast_text_model = load_fast_text_model(train_sentences)\n",
    "# godin_model = load_godin_word_embedding(\"word_embeddings/word2vec_twitter_model.bin\")\n",
    "word2vec_model= load_google_word2vec('word_embeddings/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding_matrix_glove = get_GloVe_embedding_matrix(glove_model)\n",
    "embedding_matrix_word2vec = get_word2vec_embedding_matrix(word2vec_model)\n",
    "# embedding_matrix_fast_text = get_fast_text_matrix(fast_text_model)\n",
    "# embedding_matrix_godin = get_godin_embedding_matrix(godin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parameters():\n",
    "    #range values\n",
    "    para_n_dense = [100,200,300,400]\n",
    "    para_n_filters = [100,200,300,400]\n",
    "    para_filter_size = [1,2,3,4,5,6]\n",
    "#     para_em = ['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free']\n",
    "    para_em = ['embedding_matrix_word2vec']\n",
    "    para_free_em_dim = [100,300,400]\n",
    "    para_em_trainable_flag = [True,False]\n",
    "    para_batch_size = [8,16,32,64]\n",
    "#     para_epoc = [10,30,60,100]\n",
    "    para_epoc = [1]\n",
    "#     para_batch_size = [64]\n",
    "    #selecting_random_value\n",
    "    parameters = {\"n_dense\": choice(para_n_dense),\n",
    "            \"dropout\": uniform(0.4, 0.9),\n",
    "            \"learning_rate\": uniform(0.0001, 0.01),\n",
    "            \"n_filters\": choice(para_n_filters),\n",
    "            \"filter_size_c1\": choice(para_filter_size),\n",
    "            \"filter_size_c2\": choice(para_filter_size),\n",
    "            \"filter_size_c3\": choice(para_filter_size),\n",
    "            \"em_c1\": choice(para_em),\n",
    "            \"em_c2\": choice(para_em),\n",
    "            \"em_c3\": choice(para_em),\n",
    "            \"free_em_dim\": choice(para_free_em_dim),\n",
    "            \"em_trainable_flag_c1\": choice(para_em_trainable_flag),\n",
    "            \"em_trainable_flag_c2\": choice(para_em_trainable_flag),\n",
    "            \"em_trainable_flag_c3\": choice(para_em_trainable_flag),\n",
    "            \"batch\": choice(para_batch_size),\n",
    "            \"epoch\": choice(para_epoc)\n",
    "        }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = 1\n",
    "record = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_models = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model number 1\n",
      "{'n_dense': 300, 'dropout': 0.6185354212195162, 'learning_rate': 0.004108416629722289, 'n_filters': 400, 'filter_size_c1': 4, 'filter_size_c2': 4, 'filter_size_c3': 2, 'em_c1': 'embedding_matrix_word2vec', 'em_c2': 'embedding_matrix_word2vec', 'em_c3': 'embedding_matrix_word2vec', 'free_em_dim': 300, 'em_trainable_flag_c1': False, 'em_trainable_flag_c2': False, 'em_trainable_flag_c3': True, 'batch': 16, 'epoch': 1}\n",
      "k fold validation itr == 1\n",
      "Epoch 1/1\n",
      "7284/7284 [==============================] - 320s 44ms/step - loss: 0.5143 - acc: 0.7525\n",
      "(1, 1)\n",
      "(1, 0)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(0, 0)\n",
      "(2, 2)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(0, 0)\n",
      "(2, 2)\n",
      "(1, 2)\n",
      "(0, 0)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(1, 2)\n",
      "(0, 0)\n",
      "(1, 0)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(0, 0)\n",
      "(2, 2)\n",
      "(1, 0)\n",
      "(2, 2)\n",
      "(1, 0)\n",
      "(1, 2)\n",
      "(2, 2)\n",
      "(1, 1)\n",
      "(0, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(2, 2)\n",
      "(2, 0)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(1, 2)\n",
      "(2, 2)\n",
      "(0, 2)\n",
      "(1, 1)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "0.5419602818705958 0.656832298136646 0.4612868047982552\n",
      "k fold validation itr == 2\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "for _ in range(number_of_models):\n",
    "    itr = 1\n",
    "    f1_record = []\n",
    "    p_record = []\n",
    "    r_record = []\n",
    "    itr_record = {}\n",
    "    parameters = get_parameters()\n",
    "#     parameters = {\n",
    "#             \"n_dense\": 400,\n",
    "#             \"dropout\": 0.5777195655120914,\n",
    "#             \"learning_rate\": 0.0071353667446707675,\n",
    "#             \"n_filters\": 100,\n",
    "#             \"filter_size_c1\": 6,\n",
    "#             \"filter_size_c2\": 4,\n",
    "#             \"filter_size_c3\": 4,\n",
    "#             \"em_c1\": \"embedding_matrix_word2vec\",\n",
    "#             \"em_c2\": \"embedding_matrix_word2vec\",\n",
    "#             \"em_c3\": \"embedding_matrix_word2vec\",\n",
    "#             \"free_em_dim\": 400,\n",
    "#             \"em_trainable_flag_c1\": False,\n",
    "#             \"em_trainable_flag_c2\": True,\n",
    "#             \"em_trainable_flag_c3\": False,\n",
    "#             \"batch\": 16,\n",
    "#             \"epoch\": 1\n",
    "#         }\n",
    "    print(\"model number {0}\".format(key))\n",
    "    print(parameters)\n",
    "    for train,test in kfold.split(trainX,trainY):\n",
    "        print(\"k fold validation itr == {0}\".format(itr))\n",
    "        X = trainX[train]\n",
    "        Y = to_categorical(trainY[train],num_classes=3)\n",
    "        X_ = trainX[test]\n",
    "        Y_ = list(trainY[test])\n",
    "        model = define_model(length = max_len,\n",
    "                             vocab_size=vocab_size,\n",
    "                             n_dense = parameters[\"n_dense\"],\n",
    "                             dropout = parameters[\"dropout\"],\n",
    "                             learning_rate = parameters[\"learning_rate\"],\n",
    "                             n_filters = parameters[\"n_filters\"],\n",
    "                             filter_size_c1 = parameters[\"filter_size_c1\"],\n",
    "                             filter_size_c2 = parameters[\"filter_size_c2\"],\n",
    "                             filter_size_c3 = parameters[\"filter_size_c3\"],\n",
    "                             em_c1 = parameters[\"em_c1\"],\n",
    "                             em_c2 = parameters[\"em_c1\"],\n",
    "                             em_c3 = parameters[\"em_c1\"],\n",
    "                             free_em_dim = parameters[\"free_em_dim\"],\n",
    "                             em_trainable_flag_c1 = parameters[\"em_trainable_flag_c1\"],\n",
    "                             em_trainable_flag_c2 = parameters[\"em_trainable_flag_c2\"],\n",
    "                             em_trainable_flag_c3 = parameters[\"em_trainable_flag_c3\"])\n",
    "        history = model.fit([X,X,X],Y,epochs=parameters[\"epoch\"],batch_size=parameters[\"batch\"])\n",
    "        pred = model.predict([X_,X_,X_])\n",
    "        pred_labels = [x.argmax() for x in pred]\n",
    "        for foo in zip(Y_[:50],pred_labels[:50]):\n",
    "            print(foo)\n",
    "        f1 = f1_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "        p = precision_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "        r = recall_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "        print(f1,p,r)\n",
    "        f1_record.append(f1)\n",
    "        p_record.append(p)\n",
    "        r_record.append(r)\n",
    "        itr_record[itr] = {}\n",
    "        itr_record[itr][\"f1\"] = f1\n",
    "        itr_record[itr][\"p\"] = p\n",
    "        itr_record[itr][\"r\"] = r\n",
    "        model.save('models/'+str(key)+'_'+str(itr)+'.h5')\n",
    "        itr+=1\n",
    "    record[key] = {}\n",
    "    record[key][\"parameter\"] = parameters\n",
    "    record[key][\"mean_f1\"] = np.mean(f1_record)\n",
    "    record[key][\"itr_record\"] = itr_record\n",
    "\n",
    "    with open(\"models/record.json\",'w')as fout:\n",
    "        json.dump(record,fout,indent=4)\n",
    "    key+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
