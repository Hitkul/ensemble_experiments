{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "from gensim.models import KeyedVectors\n",
    "import word2vecReader as godin_embedding\n",
    "import fasttext\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,Embedding\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from random import uniform,choice\n",
    "from os import remove\n",
    "import re\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_file(filename):\n",
    "    with open(filename,'r', errors='ignore') as fin:\n",
    "        lines = fin.readlines()\n",
    "    \n",
    "    label = [int(x.split()[0]) for x in lines]\n",
    "    sentence = [' '.join(x.split()[1:]) for x in lines]\n",
    "    return label,sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train,sentence_train = load_data_from_file('dataset/trec/TREC.train.all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# label_dev,sentence_dev = load_data_from_file('dataset/sst1/stsa.fine.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_sentences = sentence_train+sentence_dev\n",
    "# train_labels = label_train+label_dev\n",
    "train_sentences = sentence_train\n",
    "train_labels = label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5452, 5452)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences),len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_classes = len(set(train_labels))\n",
    "number_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1162\n",
      "1250\n",
      "86\n",
      "1223\n",
      "835\n",
      "896\n"
     ]
    }
   ],
   "source": [
    "for ele in list(set(train_labels)):\n",
    "    print(train_labels.count(ele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #for PYTHON 2.7\n",
    "    #tokens = [w.translate(None, string.punctuation) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train data\n"
     ]
    }
   ],
   "source": [
    "print(\"cleaning train data\")\n",
    "trainX = [clean_sentence(s) for s in train_sentences]\n",
    "trainY = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = [len(s.split()) for s in trainX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAAIMCAYAAAAKDkGtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmCYoYnRoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6210198d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length)\n",
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading GloVe embedding\n",
    "def load_GloVe_embedding(file_name):\n",
    "    print('Loading GloVe word vectors.')\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "def get_GloVe_embedding_matrix(embeddings_index):\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fast text word embedding\n",
    "def load_fast_text_model(sentences):\n",
    "    try:\n",
    "        m = fasttext.load_model('word_embeddings/fast_text_model.bin')\n",
    "        print(\"trained model loaded\")\n",
    "        return m\n",
    "    except:\n",
    "        print(\"traning new model\")\n",
    "        with open('temp_file.txt','w') as temp_file:\n",
    "            for sentence in sentences:\n",
    "                temp_file.write(sentence)\n",
    "        m = fasttext.cbow('temp_file.txt','word_embeddings/fast_text_model')\n",
    "        remove('temp_file.txt')\n",
    "        print('model trained')\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fast_text_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading godin word embedding\n",
    "def load_godin_word_embedding(path):\n",
    "    print(\"Loading Goding model.\")\n",
    "    return godin_embedding.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_godin_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,400))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading Google Word2Vec\n",
    "def load_google_word2vec(file_name):\n",
    "    print(\"Loading google news word2vec\")\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(length,vocab_size,n_dense,dropout,learning_rate,n_filters,filter_size,em,free_em_dim,number_of_classes):\n",
    "    inputs = Input(shape=(length,))\n",
    "    if em == 'free':\n",
    "        embedding = Embedding(vocab_size, free_em_dim)(inputs)\n",
    "    else:\n",
    "        embedding = Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=length,trainable = False)(inputs)\n",
    "    \n",
    "    conv = Conv1D(filters=n_filters, kernel_size=filter_size, activation='relu')(embedding)\n",
    "    drop = Dropout(dropout)(conv)\n",
    "    pool = MaxPooling1D(pool_size=2)(drop)\n",
    "    flat = Flatten()(pool)\n",
    "    # interpretation\n",
    "    dense = Dense(n_dense, activation='relu')(flat)\n",
    "    outputs = Dense(number_of_classes, activation='sigmoid')(dense)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # compile\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 12\n",
      "Vocabulary size: 8201\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(trainX)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % max_len)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "trainX = encode_text(tokenizer, trainX, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google news word2vec\n"
     ]
    }
   ],
   "source": [
    "# glove_model = load_GloVe_embedding('word_embeddings/glove.6B.300d.txt')\n",
    "# fast_text_model = load_fast_text_model(train_sentences)\n",
    "# godin_model = load_godin_word_embedding(\"word_embeddings/word2vec_twitter_model.bin\")\n",
    "word2vec_model= load_google_word2vec('word_embeddings/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding_matrix_glove = get_GloVe_embedding_matrix(glove_model)\n",
    "embedding_matrix_word2vec = get_word2vec_embedding_matrix(word2vec_model)\n",
    "# embedding_matrix_fast_text = get_fast_text_matrix(fast_text_model)\n",
    "# embedding_matrix_godin = get_godin_embedding_matrix(godin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',name='learning_rate')\n",
    "\n",
    "para_dropout = Real(low=0.4, high=0.9,name = 'dropout')\n",
    "\n",
    "para_n_dense = Categorical(categories=[100,200,300,400], name='n_dense')\n",
    "\n",
    "para_n_filters = Categorical(categories=[10,32,64,100,200],name='n_filters')\n",
    "\n",
    "para_filter_size = Integer(low=1,high=8,name = 'filter_size')\n",
    "para_em = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em')\n",
    "\n",
    "para_free_em_dim = Categorical(categories=[100,300,400],name='free_em_dim')\n",
    "\n",
    "para_batch_size = Categorical(categories=[8,16,32,64],name='batch_size')\n",
    "\n",
    "para_epoch = Categorical(categories=[10,20,50,100,150],name='epoch')\n",
    "\n",
    "\n",
    "parameters = [para_learning_rate,para_dropout,para_n_dense,para_n_filters,para_filter_size,para_em,para_free_em_dim,para_batch_size,para_epoch]\n",
    "\n",
    "default_parameters = [0.001,0.5777195655120914,100,32,3,'embedding_matrix_word2vec',100,64,10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = 1\n",
    "record = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=parameters)\n",
    "def fitness(learning_rate,dropout,n_dense,n_filters,filter_size,em,free_em_dim,batch_size,epoch):\n",
    "    global key\n",
    "    global record\n",
    "    global number_of_classes\n",
    "    print('-----------------------------combination no={0}------------------'.format(key))\n",
    "    parameters = {\n",
    "            \"n_dense\": n_dense,\n",
    "            \"dropout\": dropout,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"n_filters\": n_filters,\n",
    "            \"filter_size\": filter_size,\n",
    "            \"em\": em,\n",
    "            \"free_em_dim\": free_em_dim,\n",
    "            \"batch\": batch_size,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "    \n",
    "    pprint(parameters)\n",
    "    \n",
    "    itr = 1\n",
    "    acc_record = []\n",
    "    itr_record = {}\n",
    "    for train,test in kfold.split(trainX,trainY):\n",
    "        print(\"k fold validation itr == {0}\".format(itr))\n",
    "        X = trainX[train]\n",
    "        Y = to_categorical(trainY[train],num_classes=number_of_classes)\n",
    "        X_ = trainX[test]\n",
    "        Y_ = to_categorical(trainY[test],num_classes=number_of_classes)\n",
    "        model = define_model(length = max_len,\n",
    "                             vocab_size=vocab_size,\n",
    "                             n_dense = parameters[\"n_dense\"],\n",
    "                             dropout = parameters[\"dropout\"],\n",
    "                             learning_rate = parameters[\"learning_rate\"],\n",
    "                             n_filters = parameters[\"n_filters\"],\n",
    "                             filter_size = parameters[\"filter_size\"],\n",
    "                             em = parameters[\"em\"],\n",
    "                             free_em_dim = parameters[\"free_em_dim\"],\n",
    "                             number_of_classes = number_of_classes)\n",
    "        history = model.fit(X,Y,validation_data = [X_,Y_],epochs=parameters[\"epoch\"],batch_size=parameters[\"batch\"])\n",
    "        acc = history.history['val_acc'][-1]\n",
    "        print(acc)\n",
    "        acc_record.append(acc)\n",
    "        itr_record[itr] = {}\n",
    "        itr_record[itr][\"acc\"] = acc\n",
    "        model.save('models/'+str(key)+'_'+str(itr)+'.h5')\n",
    "        itr+=1\n",
    "    record[key] = {}\n",
    "    record[key][\"parameter\"] = parameters\n",
    "    mean_acc = np.mean(acc_record)\n",
    "    record[key][\"mean_acc\"] = mean_acc\n",
    "    record[key][\"itr_record\"] = itr_record\n",
    "    with open(\"models/record.json\",'w')as fout:\n",
    "        json.dump(record,fout,indent=4)\n",
    "    key+=1\n",
    "    \n",
    "    del model\n",
    "    K.clear_session()\n",
    "    \n",
    "    return -mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------combination no=1------------------\n",
      "{'batch': 64,\n",
      " 'dropout': 0.5777195655120914,\n",
      " 'em': 'embedding_matrix_word2vec',\n",
      " 'epoch': 10,\n",
      " 'filter_size': 3,\n",
      " 'free_em_dim': 100,\n",
      " 'learning_rate': 0.001,\n",
      " 'n_dense': 100,\n",
      " 'n_filters': 32}\n",
      "k fold validation itr == 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 12, 300)           2460300   \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 10, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 2,505,838\n",
      "Trainable params: 45,538\n",
      "Non-trainable params: 2,460,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4904 samples, validate on 548 samples\n",
      "Epoch 1/10\n",
      "4904/4904 [==============================] - 1s 290us/step - loss: 0.4556 - acc: 0.8126 - val_loss: 0.3823 - val_acc: 0.8412\n",
      "Epoch 2/10\n",
      "4904/4904 [==============================] - 1s 136us/step - loss: 0.3220 - acc: 0.8649 - val_loss: 0.2733 - val_acc: 0.8978\n",
      "Epoch 3/10\n",
      "4904/4904 [==============================] - 1s 134us/step - loss: 0.2413 - acc: 0.9003 - val_loss: 0.2197 - val_acc: 0.9200\n",
      "Epoch 4/10\n",
      "4904/4904 [==============================] - 1s 148us/step - loss: 0.1991 - acc: 0.9199 - val_loss: 0.1912 - val_acc: 0.9270\n",
      "Epoch 5/10\n",
      "4904/4904 [==============================] - 1s 161us/step - loss: 0.1763 - acc: 0.9301 - val_loss: 0.1770 - val_acc: 0.9328\n",
      "Epoch 6/10\n",
      "4904/4904 [==============================] - 1s 142us/step - loss: 0.1529 - acc: 0.9412 - val_loss: 0.1649 - val_acc: 0.9364\n",
      "Epoch 7/10\n",
      "4904/4904 [==============================] - 1s 138us/step - loss: 0.1438 - acc: 0.9450 - val_loss: 0.1587 - val_acc: 0.9386\n",
      "Epoch 8/10\n",
      "4904/4904 [==============================] - 1s 143us/step - loss: 0.1304 - acc: 0.9505 - val_loss: 0.1506 - val_acc: 0.9383\n",
      "Epoch 9/10\n",
      "4904/4904 [==============================] - 1s 137us/step - loss: 0.1223 - acc: 0.9545 - val_loss: 0.1514 - val_acc: 0.9413\n",
      "Epoch 10/10\n",
      "4904/4904 [==============================] - 1s 137us/step - loss: 0.1142 - acc: 0.9568 - val_loss: 0.1438 - val_acc: 0.9413\n",
      "0.94130169910236\n",
      "k fold validation itr == 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 12, 300)           2460300   \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 10, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 2,505,838\n",
      "Trainable params: 45,538\n",
      "Non-trainable params: 2,460,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4904 samples, validate on 548 samples\n",
      "Epoch 1/10\n",
      "4904/4904 [==============================] - 1s 240us/step - loss: 0.4523 - acc: 0.8109 - val_loss: 0.3877 - val_acc: 0.8382\n",
      "Epoch 2/10\n",
      "4904/4904 [==============================] - 1s 141us/step - loss: 0.3276 - acc: 0.8607 - val_loss: 0.2895 - val_acc: 0.8884\n",
      "Epoch 3/10\n",
      "4904/4904 [==============================] - 1s 145us/step - loss: 0.2525 - acc: 0.8968 - val_loss: 0.2333 - val_acc: 0.9121\n",
      "Epoch 4/10\n",
      "4904/4904 [==============================] - 1s 144us/step - loss: 0.2149 - acc: 0.9138 - val_loss: 0.2024 - val_acc: 0.9243\n",
      "Epoch 5/10\n",
      "4904/4904 [==============================] - 1s 153us/step - loss: 0.1820 - acc: 0.9289 - val_loss: 0.1804 - val_acc: 0.9343\n",
      "Epoch 6/10\n",
      "4904/4904 [==============================] - 1s 152us/step - loss: 0.1652 - acc: 0.9368 - val_loss: 0.1724 - val_acc: 0.9316\n",
      "Epoch 7/10\n",
      "4904/4904 [==============================] - 1s 168us/step - loss: 0.1494 - acc: 0.9433 - val_loss: 0.1581 - val_acc: 0.9373\n",
      "Epoch 8/10\n",
      "4904/4904 [==============================] - 1s 156us/step - loss: 0.1380 - acc: 0.9475 - val_loss: 0.1538 - val_acc: 0.9386\n",
      "Epoch 9/10\n",
      "4904/4904 [==============================] - 1s 151us/step - loss: 0.1311 - acc: 0.9498 - val_loss: 0.1515 - val_acc: 0.9383\n",
      "Epoch 10/10\n",
      "4904/4904 [==============================] - 1s 154us/step - loss: 0.1193 - acc: 0.9536 - val_loss: 0.1492 - val_acc: 0.9392\n",
      "0.9391727434457654\n",
      "k fold validation itr == 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 12, 300)           2460300   \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 10, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 2,505,838\n",
      "Trainable params: 45,538\n",
      "Non-trainable params: 2,460,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4905 samples, validate on 547 samples\n",
      "Epoch 1/10\n",
      "4905/4905 [==============================] - 1s 254us/step - loss: 0.4359 - acc: 0.8246 - val_loss: 0.3598 - val_acc: 0.8525\n",
      "Epoch 2/10\n",
      "4905/4905 [==============================] - 1s 148us/step - loss: 0.3056 - acc: 0.8732 - val_loss: 0.2571 - val_acc: 0.8991\n",
      "Epoch 3/10\n",
      "4905/4905 [==============================] - 1s 144us/step - loss: 0.2419 - acc: 0.8996 - val_loss: 0.2069 - val_acc: 0.9205\n",
      "Epoch 4/10\n",
      "4905/4905 [==============================] - 1s 151us/step - loss: 0.2017 - acc: 0.9181 - val_loss: 0.1778 - val_acc: 0.9369\n",
      "Epoch 5/10\n",
      "4905/4905 [==============================] - 1s 146us/step - loss: 0.1784 - acc: 0.9301 - val_loss: 0.1576 - val_acc: 0.9415\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4905/4905 [==============================] - 1s 132us/step - loss: 0.1602 - acc: 0.9366 - val_loss: 0.1419 - val_acc: 0.9476\n",
      "Epoch 7/10\n",
      "4905/4905 [==============================] - 1s 133us/step - loss: 0.1492 - acc: 0.9412 - val_loss: 0.1336 - val_acc: 0.9509\n",
      "Epoch 8/10\n",
      "4905/4905 [==============================] - 1s 130us/step - loss: 0.1383 - acc: 0.9456 - val_loss: 0.1250 - val_acc: 0.9537\n",
      "Epoch 9/10\n",
      "4905/4905 [==============================] - 1s 125us/step - loss: 0.1238 - acc: 0.9518 - val_loss: 0.1199 - val_acc: 0.9549\n",
      "Epoch 10/10\n",
      "4905/4905 [==============================] - 1s 131us/step - loss: 0.1175 - acc: 0.9548 - val_loss: 0.1153 - val_acc: 0.9558\n",
      "0.9558196115755294\n",
      "k fold validation itr == 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "embedding_14 (Embedding)     (None, 12, 300)           2460300   \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 10, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 2,505,838\n",
      "Trainable params: 45,538\n",
      "Non-trainable params: 2,460,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4906 samples, validate on 546 samples\n",
      "Epoch 1/10\n",
      "4906/4906 [==============================] - 1s 242us/step - loss: 0.4454 - acc: 0.8102 - val_loss: 0.3668 - val_acc: 0.8471\n",
      "Epoch 2/10\n",
      "4906/4906 [==============================] - 1s 134us/step - loss: 0.3043 - acc: 0.8706 - val_loss: 0.2623 - val_acc: 0.8984\n",
      "Epoch 3/10\n",
      "4906/4906 [==============================] - 1s 130us/step - loss: 0.2364 - acc: 0.9024 - val_loss: 0.2183 - val_acc: 0.9164\n",
      "Epoch 4/10\n",
      "4906/4906 [==============================] - 1s 133us/step - loss: 0.2013 - acc: 0.9177 - val_loss: 0.1940 - val_acc: 0.9264\n",
      "Epoch 5/10\n",
      "4906/4906 [==============================] - 1s 133us/step - loss: 0.1795 - acc: 0.9274 - val_loss: 0.1798 - val_acc: 0.9332\n",
      "Epoch 6/10\n",
      "4906/4906 [==============================] - 1s 131us/step - loss: 0.1559 - acc: 0.9396 - val_loss: 0.1674 - val_acc: 0.9386\n",
      "Epoch 7/10\n",
      "4906/4906 [==============================] - 1s 133us/step - loss: 0.1468 - acc: 0.9412 - val_loss: 0.1627 - val_acc: 0.9408\n",
      "Epoch 8/10\n",
      "4906/4906 [==============================] - 1s 136us/step - loss: 0.1313 - acc: 0.9484 - val_loss: 0.1598 - val_acc: 0.9435\n",
      "Epoch 9/10\n",
      "4906/4906 [==============================] - 1s 136us/step - loss: 0.1231 - acc: 0.9526 - val_loss: 0.1574 - val_acc: 0.9447\n",
      "Epoch 10/10\n",
      "4906/4906 [==============================] - 1s 133us/step - loss: 0.1123 - acc: 0.9571 - val_loss: 0.1544 - val_acc: 0.9420\n",
      "0.9420024519001607\n",
      "k fold validation itr == 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "embedding_15 (Embedding)     (None, 12, 300)           2460300   \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 10, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 2,505,838\n",
      "Trainable params: 45,538\n",
      "Non-trainable params: 2,460,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4906 samples, validate on 546 samples\n",
      "Epoch 1/10\n",
      "4906/4906 [==============================] - 1s 252us/step - loss: 0.4306 - acc: 0.8284 - val_loss: 0.3626 - val_acc: 0.8477\n",
      "Epoch 2/10\n",
      "4906/4906 [==============================] - 1s 134us/step - loss: 0.3043 - acc: 0.8722 - val_loss: 0.2672 - val_acc: 0.9005\n",
      "Epoch 3/10\n",
      "4906/4906 [==============================] - 1s 136us/step - loss: 0.2367 - acc: 0.9020 - val_loss: 0.2221 - val_acc: 0.9158\n",
      "Epoch 4/10\n",
      "4906/4906 [==============================] - 1s 133us/step - loss: 0.2026 - acc: 0.9185 - val_loss: 0.1956 - val_acc: 0.9234\n",
      "Epoch 5/10\n",
      "4906/4906 [==============================] - 1s 148us/step - loss: 0.1763 - acc: 0.9294 - val_loss: 0.1759 - val_acc: 0.9344\n",
      "Epoch 6/10\n",
      "4906/4906 [==============================] - 1s 137us/step - loss: 0.1590 - acc: 0.9371 - val_loss: 0.1611 - val_acc: 0.9405\n",
      "Epoch 7/10\n",
      "4906/4906 [==============================] - 1s 141us/step - loss: 0.1444 - acc: 0.9448 - val_loss: 0.1547 - val_acc: 0.9389\n",
      "Epoch 8/10\n",
      "4906/4906 [==============================] - 1s 135us/step - loss: 0.1352 - acc: 0.9476 - val_loss: 0.1520 - val_acc: 0.9396\n",
      "Epoch 9/10\n",
      "4906/4906 [==============================] - 1s 142us/step - loss: 0.1196 - acc: 0.9527 - val_loss: 0.1437 - val_acc: 0.9432\n",
      "Epoch 10/10\n",
      "4906/4906 [==============================] - 1s 141us/step - loss: 0.1166 - acc: 0.9558 - val_loss: 0.1416 - val_acc: 0.9423\n",
      "0.9423076990759853\n",
      "k fold validation itr == 6\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "embedding_16 (Embedding)     (None, 12, 300)           2460300   \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 10, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 2,505,838\n",
      "Trainable params: 45,538\n",
      "Non-trainable params: 2,460,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4907 samples, validate on 545 samples\n",
      "Epoch 1/10\n",
      "4907/4907 [==============================] - 1s 264us/step - loss: 0.4352 - acc: 0.8266 - val_loss: 0.3658 - val_acc: 0.8456\n",
      "Epoch 2/10\n",
      "4907/4907 [==============================] - 1s 136us/step - loss: 0.3075 - acc: 0.8701 - val_loss: 0.2613 - val_acc: 0.8997\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4907/4907 [==============================] - 1s 129us/step - loss: 0.2397 - acc: 0.9002 - val_loss: 0.2159 - val_acc: 0.9177\n",
      "Epoch 4/10\n",
      "4907/4907 [==============================] - 1s 128us/step - loss: 0.2036 - acc: 0.9185 - val_loss: 0.1877 - val_acc: 0.9306\n",
      "Epoch 5/10\n",
      "4907/4907 [==============================] - 1s 130us/step - loss: 0.1766 - acc: 0.9298 - val_loss: 0.1683 - val_acc: 0.9425\n",
      "Epoch 6/10\n",
      "4907/4907 [==============================] - 1s 130us/step - loss: 0.1601 - acc: 0.9357 - val_loss: 0.1572 - val_acc: 0.9388\n",
      "Epoch 7/10\n",
      "4907/4907 [==============================] - 1s 130us/step - loss: 0.1468 - acc: 0.9414 - val_loss: 0.1480 - val_acc: 0.9492\n",
      "Epoch 8/10\n",
      "4907/4907 [==============================] - 1s 133us/step - loss: 0.1343 - acc: 0.9472 - val_loss: 0.1429 - val_acc: 0.9474\n",
      "Epoch 9/10\n",
      "4907/4907 [==============================] - 1s 128us/step - loss: 0.1256 - acc: 0.9513 - val_loss: 0.1373 - val_acc: 0.9498\n",
      "Epoch 10/10\n",
      "4907/4907 [==============================] - 1s 131us/step - loss: 0.1171 - acc: 0.9555 - val_loss: 0.1360 - val_acc: 0.9459\n",
      "0.9458715607266908\n",
      "k fold validation itr == 7\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "embedding_17 (Embedding)     (None, 12, 300)           2460300   \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 10, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 2,505,838\n",
      "Trainable params: 45,538\n",
      "Non-trainable params: 2,460,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4909 samples, validate on 543 samples\n",
      "Epoch 1/10\n",
      "4909/4909 [==============================] - 1s 258us/step - loss: 0.4365 - acc: 0.8261 - val_loss: 0.3643 - val_acc: 0.8475\n",
      "Epoch 2/10\n",
      "4909/4909 [==============================] - 1s 130us/step - loss: 0.3045 - acc: 0.8724 - val_loss: 0.2532 - val_acc: 0.9125\n",
      "Epoch 3/10\n",
      "4909/4909 [==============================] - 1s 133us/step - loss: 0.2351 - acc: 0.9028 - val_loss: 0.2062 - val_acc: 0.9303\n",
      "Epoch 4/10\n",
      "4909/4909 [==============================] - 1s 133us/step - loss: 0.2022 - acc: 0.9181 - val_loss: 0.1826 - val_acc: 0.9362\n",
      "Epoch 5/10\n",
      "4909/4909 [==============================] - 1s 130us/step - loss: 0.1751 - acc: 0.9315 - val_loss: 0.1651 - val_acc: 0.9377\n",
      "Epoch 6/10\n",
      "4909/4909 [==============================] - 1s 133us/step - loss: 0.1580 - acc: 0.9389 - val_loss: 0.1562 - val_acc: 0.9395\n",
      "Epoch 7/10\n",
      "4909/4909 [==============================] - 1s 134us/step - loss: 0.1423 - acc: 0.9442 - val_loss: 0.1500 - val_acc: 0.9398\n",
      "Epoch 8/10\n",
      "4909/4909 [==============================] - 1s 134us/step - loss: 0.1351 - acc: 0.9487 - val_loss: 0.1443 - val_acc: 0.9414\n",
      "Epoch 9/10\n",
      "4909/4909 [==============================] - 1s 133us/step - loss: 0.1253 - acc: 0.9533 - val_loss: 0.1408 - val_acc: 0.9463\n",
      "Epoch 10/10\n",
      "4909/4909 [==============================] - 1s 133us/step - loss: 0.1163 - acc: 0.9554 - val_loss: 0.1367 - val_acc: 0.9469\n",
      "0.9468999488577658\n",
      "k fold validation itr == 8\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, 12, 300)           2460300   \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 10, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 2,505,838\n",
      "Trainable params: 45,538\n",
      "Non-trainable params: 2,460,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4909 samples, validate on 543 samples\n",
      "Epoch 1/10\n",
      "4909/4909 [==============================] - 1s 274us/step - loss: 0.4521 - acc: 0.8101 - val_loss: 0.3572 - val_acc: 0.8530\n",
      "Epoch 2/10\n",
      "4909/4909 [==============================] - 1s 137us/step - loss: 0.3068 - acc: 0.8717 - val_loss: 0.2578 - val_acc: 0.8963\n",
      "Epoch 3/10\n",
      "4909/4909 [==============================] - 1s 155us/step - loss: 0.2384 - acc: 0.9003 - val_loss: 0.2102 - val_acc: 0.9184\n",
      "Epoch 4/10\n",
      "4909/4909 [==============================] - 1s 138us/step - loss: 0.2018 - acc: 0.9203 - val_loss: 0.1826 - val_acc: 0.9340\n",
      "Epoch 5/10\n",
      "4909/4909 [==============================] - 1s 146us/step - loss: 0.1787 - acc: 0.9291 - val_loss: 0.1634 - val_acc: 0.9408\n",
      "Epoch 6/10\n",
      "4909/4909 [==============================] - 1s 141us/step - loss: 0.1599 - acc: 0.9373 - val_loss: 0.1512 - val_acc: 0.9429\n",
      "Epoch 7/10\n",
      "4909/4909 [==============================] - 1s 153us/step - loss: 0.1456 - acc: 0.9420 - val_loss: 0.1444 - val_acc: 0.9487\n",
      "Epoch 8/10\n",
      "4909/4909 [==============================] - 1s 144us/step - loss: 0.1374 - acc: 0.9457 - val_loss: 0.1368 - val_acc: 0.9512\n",
      "Epoch 9/10\n",
      "4909/4909 [==============================] - 1s 153us/step - loss: 0.1273 - acc: 0.9511 - val_loss: 0.1326 - val_acc: 0.9490\n",
      "Epoch 10/10\n",
      "4909/4909 [==============================] - 1s 156us/step - loss: 0.1160 - acc: 0.9546 - val_loss: 0.1268 - val_acc: 0.9537\n",
      "0.9536525349151583\n",
      "k fold validation itr == 9\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "embedding_19 (Embedding)     (None, 12, 300)           2460300   \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 10, 32)            28832     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 2,505,838\n",
      "Trainable params: 45,538\n",
      "Non-trainable params: 2,460,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4909 samples, validate on 543 samples\n",
      "Epoch 1/10\n",
      "4909/4909 [==============================] - 2s 323us/step - loss: 0.4583 - acc: 0.8081 - val_loss: 0.3699 - val_acc: 0.8462\n",
      "Epoch 2/10\n",
      "4909/4909 [==============================] - 1s 132us/step - loss: 0.3235 - acc: 0.8621 - val_loss: 0.2748 - val_acc: 0.8950\n",
      "Epoch 3/10\n",
      "4909/4909 [==============================] - 1s 156us/step - loss: 0.2558 - acc: 0.8928 - val_loss: 0.2230 - val_acc: 0.9177\n",
      "Epoch 4/10\n",
      "4909/4909 [==============================] - 1s 133us/step - loss: 0.2129 - acc: 0.9129 - val_loss: 0.1884 - val_acc: 0.9328\n",
      "Epoch 5/10\n",
      "4909/4909 [==============================] - 1s 143us/step - loss: 0.1828 - acc: 0.9246 - val_loss: 0.1671 - val_acc: 0.9383\n",
      "Epoch 6/10\n",
      "4909/4909 [==============================] - 1s 137us/step - loss: 0.1637 - acc: 0.9350 - val_loss: 0.1559 - val_acc: 0.9432\n",
      "Epoch 7/10\n",
      "1600/4909 [========>.....................] - ETA: 0s - loss: 0.1447 - acc: 0.9427"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-f5b44a4ceba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0macq_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'EI'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                             x0=default_parameters)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# User suggested points at which to evaluate the objective first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my0\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mn_calls\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-241-a6425669b411>\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(learning_rate, dropout, n_dense, n_filters, filter_size, em, free_em_dim, batch_size, epoch)\u001b[0m\n\u001b[1;32m     38\u001b[0m                              \u001b[0mfree_em_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"free_em_dim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                              number_of_classes = number_of_classes)\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters,\n",
    "                            acq_func='EI',\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_parameters():\n",
    "#     #range values\n",
    "#     para_n_dense = [100,200,300,400]\n",
    "#     para_n_filters = [100,200,300,400]\n",
    "#     para_filter_size = [1,2,3,4,5,6]\n",
    "# #     para_em = ['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free']\n",
    "#     para_em = ['embedding_matrix_word2vec']\n",
    "#     para_free_em_dim = [100,300,400]\n",
    "#     para_em_trainable_flag = [True,False]\n",
    "#     para_batch_size = [8,16,32,64]\n",
    "# #     para_epoc = [10,30,60,100]\n",
    "#     para_epoc = [10]\n",
    "# #     para_batch_size = [64]\n",
    "#     #selecting_random_value\n",
    "#     parameters = {\"n_dense\": choice(para_n_dense),\n",
    "#             \"dropout\": uniform(0.4, 0.9),\n",
    "#             \"learning_rate\": uniform(0.0001, 0.01),\n",
    "#             \"n_filters\": choice(para_n_filters),\n",
    "#             \"filter_size_c1\": choice(para_filter_size),\n",
    "#             \"filter_size_c2\": choice(para_filter_size),\n",
    "#             \"filter_size_c3\": choice(para_filter_size),\n",
    "#             \"em_c1\": choice(para_em),\n",
    "#             \"em_c2\": choice(para_em),\n",
    "#             \"em_c3\": choice(para_em),\n",
    "#             \"free_em_dim\": choice(para_free_em_dim),\n",
    "#             \"em_trainable_flag_c1\": choice(para_em_trainable_flag),\n",
    "#             \"em_trainable_flag_c2\": choice(para_em_trainable_flag),\n",
    "#             \"em_trainable_flag_c3\": choice(para_em_trainable_flag),\n",
    "#             \"batch\": choice(para_batch_size),\n",
    "#             \"epoch\": choice(para_epoc)\n",
    "#         }\n",
    "#     return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for _ in range(number_of_models):\n",
    "#     itr = 1\n",
    "#     f1_record = []\n",
    "#     p_record = []\n",
    "#     r_record = []\n",
    "#     itr_record = {}\n",
    "#     parameters = get_parameters()\n",
    "#     parameters = {\n",
    "#             \"n_dense\": 400,\n",
    "#             \"dropout\": 0.5777195655120914,\n",
    "#             \"learning_rate\": 0.0071353667446707675,\n",
    "#             \"n_filters\": 100,\n",
    "#             \"filter_size_c1\": 6,\n",
    "#             \"filter_size_c2\": 4,\n",
    "#             \"filter_size_c3\": 4,\n",
    "#             \"em_c1\": \"embedding_matrix_word2vec\",\n",
    "#             \"em_c2\": \"embedding_matrix_word2vec\",\n",
    "#             \"em_c3\": \"embedding_matrix_word2vec\",\n",
    "#             \"free_em_dim\": 400,\n",
    "#             \"em_trainable_flag_c1\": False,\n",
    "#             \"em_trainable_flag_c2\": True,\n",
    "#             \"em_trainable_flag_c3\": False,\n",
    "#             \"batch\": 16,\n",
    "#             \"epoch\": 1\n",
    "#         }\n",
    "#     print(\"model number {0}\".format(key))\n",
    "#     print(parameters)\n",
    "#     for train,test in kfold.split(trainX,trainY):\n",
    "#         print(\"k fold validation itr == {0}\".format(itr))\n",
    "#         X = trainX[train]\n",
    "#         Y = to_categorical(trainY[train],num_classes=3)\n",
    "#         X_ = trainX[test]\n",
    "#         Y_ = list(trainY[test])\n",
    "#         model = define_model(length = max_len,\n",
    "#                              vocab_size=vocab_size,\n",
    "#                              n_dense = parameters[\"n_dense\"],\n",
    "#                              dropout = parameters[\"dropout\"],\n",
    "#                              learning_rate = parameters[\"learning_rate\"],\n",
    "#                              n_filters = parameters[\"n_filters\"],\n",
    "#                              filter_size_c1 = parameters[\"filter_size_c1\"],\n",
    "#                              filter_size_c2 = parameters[\"filter_size_c2\"],\n",
    "#                              filter_size_c3 = parameters[\"filter_size_c3\"],\n",
    "#                              em_c1 = parameters[\"em_c1\"],\n",
    "#                              em_c2 = parameters[\"em_c1\"],\n",
    "#                              em_c3 = parameters[\"em_c1\"],\n",
    "#                              free_em_dim = parameters[\"free_em_dim\"],\n",
    "#                              em_trainable_flag_c1 = parameters[\"em_trainable_flag_c1\"],\n",
    "#                              em_trainable_flag_c2 = parameters[\"em_trainable_flag_c2\"],\n",
    "#                              em_trainable_flag_c3 = parameters[\"em_trainable_flag_c3\"])\n",
    "#         history = model.fit([X,X,X],Y,epochs=parameters[\"epoch\"],batch_size=parameters[\"batch\"])\n",
    "#         pred = model.predict([X_,X_,X_])\n",
    "#         pred_labels = [x.argmax() for x in pred]\n",
    "#         for foo in zip(Y_[:50],pred_labels[:50]):\n",
    "#             print(foo)\n",
    "#         f1 = f1_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "#         p = precision_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "#         r = recall_score(Y_,pred_labels,labels=[0,1],average='micro')\n",
    "#         print(f1,p,r)\n",
    "#         f1_record.append(f1)\n",
    "#         p_record.append(p)\n",
    "#         r_record.append(r)\n",
    "#         itr_record[itr] = {}\n",
    "#         itr_record[itr][\"f1\"] = f1\n",
    "#         itr_record[itr][\"p\"] = p\n",
    "#         itr_record[itr][\"r\"] = r\n",
    "#         model.save('models/'+str(key)+'_'+str(itr)+'.h5')\n",
    "#         itr+=1\n",
    "#     record[key] = {}\n",
    "#     record[key][\"parameter\"] = parameters\n",
    "#     record[key][\"mean_f1\"] = np.mean(f1_record)\n",
    "#     record[key][\"itr_record\"] = itr_record\n",
    "\n",
    "#     with open(\"models/record.json\",'w')as fout:\n",
    "#         json.dump(record,fout,indent=4)\n",
    "#     key+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
